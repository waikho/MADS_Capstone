{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "from psycopg.rows import dict_row\n",
    "import getdata as gd\n",
    "import modeling_main as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy.trendlabeling as tlb\n",
    "import afml.filters.filters as flt \n",
    "import afml.labeling.triplebarrier as tbar\n",
    "import afml.util.volatility as vol\n",
    "import features.bars as bars  \n",
    "import features.marketindicators as mkt\n",
    "from afml.sample_weights.attribution import get_weights_by_return\n",
    "from afml.cross_validation.cross_validation import PurgedKFold\n",
    "import crossvalidation as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init\n",
    "pgConnStr = gd.pgDictToConn(config.pgSecrets)\n",
    "tickerlst = gd.getFilteredTickerList_Daily(lowest_price=10.0, highest_price=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pick= list(np.random.choice(tickerlst, 10 if len(tickerlst) > 10 else  len(tickerlst), replace=False))\n",
    "random_pick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mm.get_alpaca_daily_data(random_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['symbol'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['symbol']=='INSI'].symbol.unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mm.get_data('TETC')\n",
    "#df.index = pd.to_datetime(df.tranx_date)\n",
    "\n",
    "index_SPY = mm.get_index(df, 'SPY')\n",
    "#index_SPY.index = pd.to_datetime(index_SPY.tranx_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-20 00:00:00-04:00 2023-03-27 00:00:00-04:00\n",
      "2021-05-20 00:00:00-04:00 2023-03-27 00:00:00-04:00\n"
     ]
    }
   ],
   "source": [
    "print(index_SPY['tranx_date'].min(), index_SPY['tranx_date'].max())\n",
    "print(df['tranx_date'].min(), df['tranx_date'].max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling as Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.tranx_date\n",
    "\n",
    "# construct the input data\n",
    "trades = df[['tranx_date', 'close', 'vol']].to_numpy()\n",
    "\n",
    "# define the dollar value to sample the data - we want to resample about 50 bars per day based on the average of last 10 days' dollar volume\n",
    "dollar_vol = (df['vol']*df['close']).resample('D').sum()[:-10].mean()/50.0\n",
    "\n",
    "# generate the dollar bars\n",
    "dollar_bars = bars.generate_dollarbars(df, dv_thres=dollar_vol) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-20 00:00:00-04:00</th>\n",
       "      <td>9.7200</td>\n",
       "      <td>9.7200</td>\n",
       "      <td>9.7200</td>\n",
       "      <td>9.7200</td>\n",
       "      <td>967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-21 00:00:00-04:00</th>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>1750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-24 00:00:00-04:00</th>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>2588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25 00:00:00-04:00</th>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>5481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26 00:00:00-04:00</th>\n",
       "      <td>9.6900</td>\n",
       "      <td>9.6900</td>\n",
       "      <td>9.6900</td>\n",
       "      <td>9.6900</td>\n",
       "      <td>2243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-17 00:00:00-04:00</th>\n",
       "      <td>10.1201</td>\n",
       "      <td>10.1201</td>\n",
       "      <td>10.1201</td>\n",
       "      <td>10.1201</td>\n",
       "      <td>17004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-20 00:00:00-04:00</th>\n",
       "      <td>10.1650</td>\n",
       "      <td>10.1650</td>\n",
       "      <td>10.1650</td>\n",
       "      <td>10.1650</td>\n",
       "      <td>310414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-21 00:00:00-04:00</th>\n",
       "      <td>10.1619</td>\n",
       "      <td>10.1619</td>\n",
       "      <td>10.1619</td>\n",
       "      <td>10.1619</td>\n",
       "      <td>2874.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-22 00:00:00-04:00</th>\n",
       "      <td>10.1600</td>\n",
       "      <td>10.1600</td>\n",
       "      <td>10.1600</td>\n",
       "      <td>10.1600</td>\n",
       "      <td>167991.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-24 00:00:00-04:00</th>\n",
       "      <td>10.1650</td>\n",
       "      <td>10.1700</td>\n",
       "      <td>10.1650</td>\n",
       "      <td>10.1700</td>\n",
       "      <td>2778.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open     high      low    close    volume\n",
       "time                                                                   \n",
       "2021-05-20 00:00:00-04:00   9.7200   9.7200   9.7200   9.7200     967.0\n",
       "2021-05-21 00:00:00-04:00   9.7000   9.7000   9.7000   9.7000    1750.0\n",
       "2021-05-24 00:00:00-04:00   9.7000   9.7000   9.7000   9.7000    2588.0\n",
       "2021-05-25 00:00:00-04:00   9.7000   9.7000   9.7000   9.7000    5481.0\n",
       "2021-05-26 00:00:00-04:00   9.6900   9.6900   9.6900   9.6900    2243.0\n",
       "...                            ...      ...      ...      ...       ...\n",
       "2023-03-17 00:00:00-04:00  10.1201  10.1201  10.1201  10.1201   17004.0\n",
       "2023-03-20 00:00:00-04:00  10.1650  10.1650  10.1650  10.1650  310414.0\n",
       "2023-03-21 00:00:00-04:00  10.1619  10.1619  10.1619  10.1619    2874.0\n",
       "2023-03-22 00:00:00-04:00  10.1600  10.1600  10.1600  10.1600  167991.0\n",
       "2023-03-24 00:00:00-04:00  10.1650  10.1700  10.1650  10.1700    2778.0\n",
       "\n",
       "[318 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dollar_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_SPY[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_bars_index = bars.transform_index_based_on_dollarbar(dollar_bars, index_SPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-20 00:00:00-04:00</th>\n",
       "      <td>411.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-21 00:00:00-04:00</th>\n",
       "      <td>416.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.98</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-24 00:00:00-04:00</th>\n",
       "      <td>417.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>419.17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25 00:00:00-04:00</th>\n",
       "      <td>420.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26 00:00:00-04:00</th>\n",
       "      <td>418.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>419.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-17 00:00:00-04:00</th>\n",
       "      <td>393.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>389.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-20 00:00:00-04:00</th>\n",
       "      <td>390.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-21 00:00:00-04:00</th>\n",
       "      <td>397.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>398.91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-22 00:00:00-04:00</th>\n",
       "      <td>398.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>392.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-24 00:00:00-04:00</th>\n",
       "      <td>391.841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>395.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open high  low   close volume\n",
       "time                                                       \n",
       "2021-05-20 00:00:00-04:00    411.8  NaN  NaN  415.15    0.0\n",
       "2021-05-21 00:00:00-04:00   416.87  NaN  NaN  414.98    0.0\n",
       "2021-05-24 00:00:00-04:00   417.34  NaN  NaN  419.17    0.0\n",
       "2021-05-25 00:00:00-04:00   420.33  NaN  NaN  418.33    0.0\n",
       "2021-05-26 00:00:00-04:00   418.87  NaN  NaN  419.04    0.0\n",
       "...                            ...  ...  ...     ...    ...\n",
       "2023-03-17 00:00:00-04:00   393.22  NaN  NaN  389.99    0.0\n",
       "2023-03-20 00:00:00-04:00    390.8  NaN  NaN  393.74    0.0\n",
       "2023-03-21 00:00:00-04:00   397.24  NaN  NaN  398.91    0.0\n",
       "2023-03-22 00:00:00-04:00   398.73  NaN  NaN  392.11    0.0\n",
       "2023-03-24 00:00:00-04:00  391.841  NaN  NaN  395.75    0.0\n",
       "\n",
       "[318 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dollar_bars_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_bars_index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.plot(dollar_bars, # the dataframe containing the OHLC (Open, High, Low and Close) data\n",
    "         type='candle', # use candlesticks \n",
    "         volume=True, # also show the volume\n",
    "         mav=(9,20,200), # use three different moving averages\n",
    "         figratio=(3,1), # set the ratio of the figure\n",
    "         style='yahoo',  # choose the yahoo style\n",
    "         title='Dollar Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then do the same for SPY index as well for calculating relative strength as feature in Model 2 \n",
    "trades_SPY = index_SPY[['datetime', 'close', 'vol']].to_numpy()\n",
    "dollar_vol_SPY = (index_SPY['vol']*index_SPY['close']).resample('D').sum()[:10].mean()/50.0\n",
    "dollar_bars_SPY = bars.generate_dollarbars(trades_SPY, frequency=dollar_vol_SPY) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Trend Scanning to label as Uptrend or Downtrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get close values of dollar bars\n",
    "time_series = dollar_bars.close.to_numpy()\n",
    "\n",
    "# set window size to calculate best slope\n",
    "window_size_max= 7\n",
    "\n",
    "# get trend scanning labels\n",
    "label_output = pd.DataFrame(tlb.get_trend_scanning_labels(time_series=time_series, \n",
    "                                             window_size_max=window_size_max, \n",
    "                                             threshold=0.0,\n",
    "                                             side='both'), \n",
    "                            index= dollar_bars.index[window_size_max-1:])\n",
    "\n",
    "# put label results back into dollar_bars Dataframe \n",
    "dollar_bars = dollar_bars.join(label_output, how='outer')\n",
    "\n",
    "# remove look-ahead bias\n",
    "dollar_bars['label'] = dollar_bars['label'].shift(1) \n",
    "dollar_bars['slope'] = dollar_bars['slope'].shift(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of uptrends vs # of downtrends\n",
    "print(dollar_bars['label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Trade Triggering Events based on Volitility Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dollar bar close data\n",
    "close = dollar_bars.close.copy()\n",
    "\n",
    "# get Daily Volatility as 50 dollar bars exponentially weighted moving std dev\n",
    "dailyVolatility = vol.getDailyVol(close, span=50)\n",
    "\n",
    "# apply cusum filter to identify events as cumulative log return passed daily volatility threshold\n",
    "tEvents = flt.cusum_filter(close, threshold=dailyVolatility.mean()*0.5, signal=None)\n",
    "\n",
    "# Define vertical barrier - this is subjective, we will close out the position if after a day\n",
    "num_days = 1\n",
    "\n",
    "t1 = tbar.add_vertical_barrier(tEvents, close, num_days=num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show volitility over time\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "dailyVolatility.plot(ax=ax)\n",
    "ax.axhline(dailyVolatility.mean(),ls='--',color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show distribution of volitility\n",
    "ax = dailyVolatility.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get side labels from trend following method for inputting into the trend following method\n",
    "side_labels = []\n",
    "\n",
    "for dt in dollar_bars.index:\n",
    "    side_labels.append(dollar_bars.loc[dt]['label'])\n",
    "\n",
    "side_labels = pd.Series(side_labels, index=dollar_bars.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trade or Not Trade labels using Triple Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upper and lower horizontal barriers\n",
    "\n",
    "# set profit and stop loss ratio, 2:1 is the common trader's ratio\n",
    "ptsl = [1.5,1]\n",
    "\n",
    "# set minimum desired return\n",
    "# requires at least 1.5% percent return fro transaction and slippage\n",
    "minRet = 0.015 \n",
    "\n",
    "# Run in single-threaded mode on Windows\n",
    "import platform, os\n",
    "if platform.system() == \"Windows\":\n",
    "    cpus = 1\n",
    "else:\n",
    "    cpus = os.cpu_count() - 1\n",
    "    \n",
    "events = tbar.get_events(dollar_bars.close, \n",
    "                         t_events=tEvents, \n",
    "                         pt_sl=ptsl, \n",
    "                         target=dailyVolatility, \n",
    "                         min_ret=minRet, \n",
    "                         num_threads=cpus, \n",
    "                         vertical_barrier_times=t1,\n",
    "                         side_prediction=side_labels).dropna()\n",
    "\n",
    "# get the triple barrier labels 1 means above minimum return, 0 otherwise.\n",
    "labels = tbar.get_bins(triple_barrier_events = events, close=close)\n",
    "\n",
    "# Drop underpopulated labels\n",
    "clean_labels  = tbar.drop_labels(labels)\n",
    "print(clean_labels.bin.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is so imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Model 1 without features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Trend Labeling method, assuming we close out our position within 24 hours and we require a 1.5:1 profit ratio. If we bet on every single uptrend or downtrend signal, our accuracy is only at 20 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(clean_labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features to the dataset for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "dollar_bars['log_ret'] = np.log(dollar_bars['close']).diff()\n",
    "\n",
    "# Momentum\n",
    "dollar_bars['mom1'] = dollar_bars['close'].pct_change(periods=1).shift(1)\n",
    "dollar_bars['mom2'] = dollar_bars['close'].pct_change(periods=2).shift(1)\n",
    "dollar_bars['mom3'] = dollar_bars['close'].pct_change(periods=3).shift(1)\n",
    "dollar_bars['mom4'] = dollar_bars['close'].pct_change(periods=4).shift(1)\n",
    "dollar_bars['mom5'] = dollar_bars['close'].pct_change(periods=5).shift(1)\n",
    "\n",
    "# Volatility\n",
    "dollar_bars['volatility_50'] = dollar_bars['log_ret'].rolling(window=50, min_periods=50, center=False).std().shift(1)\n",
    "dollar_bars['volatility_31'] = dollar_bars['log_ret'].rolling(window=31, min_periods=31, center=False).std().shift(1)\n",
    "dollar_bars['volatility_15'] = dollar_bars['log_ret'].rolling(window=15, min_periods=15, center=False).std().shift(1)\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "dollar_bars['autocorr_1'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False).shift(1)\n",
    "dollar_bars['autocorr_2'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=2), raw=False).shift(1)\n",
    "dollar_bars['autocorr_3'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False).shift(1)\n",
    "dollar_bars['autocorr_4'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=4), raw=False).shift(1)\n",
    "dollar_bars['autocorr_5'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=5), raw=False).shift(1)\n",
    "\n",
    "# Get the various log -t returns\n",
    "dollar_bars['log_t1'] = dollar_bars['log_ret'].shift(1).shift(1)\n",
    "dollar_bars['log_t2'] = dollar_bars['log_ret'].shift(2).shift(1)\n",
    "dollar_bars['log_t3'] = dollar_bars['log_ret'].shift(3).shift(1)\n",
    "dollar_bars['log_t4'] = dollar_bars['log_ret'].shift(4).shift(1)\n",
    "dollar_bars['log_t5'] = dollar_bars['log_ret'].shift(5).shift(1)\n",
    "\n",
    "# relative strength SPY at various -t\n",
    "dollar_bars['rs_SPY_t1'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(1)\n",
    "dollar_bars['rs_SPY_t2'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(2)\n",
    "dollar_bars['rs_SPY_t3'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(3)\n",
    "dollar_bars['rs_SPY_t4'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(4)\n",
    "dollar_bars['rs_SPY_t5'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(5)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Model Fitting\n",
    "\n",
    "#### Use features: volatility, serial correlation, relative strength to S&P, and original Model 1 slope and trend label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_bars = dollar_bars.join(clean_labels['bin']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dollar_bars.iloc[:, :-1]\n",
    "y = dollar_bars.iloc[:, -1]\n",
    "\n",
    "col = ['slope', 'label', 'log_ret',\n",
    "       'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'volatility_50',\n",
    "       'volatility_31', 'volatility_15', 'autocorr_1', 'autocorr_2',\n",
    "       'autocorr_3', 'autocorr_4', 'autocorr_5', 'log_t1', 'log_t2', 'log_t3',\n",
    "       'log_t4', 'log_t5', 'rs_SPY_t1', 'rs_SPY_t2', 'rs_SPY_t3', 'rs_SPY_t4',\n",
    "       'rs_SPY_t5']\n",
    "\n",
    "X = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to Label Concurrency, we will apply custom sample weights to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data sample weights\n",
    "return_based_sample_weights = get_weights_by_return(events.loc[X_train.index], dollar_bars.loc[X_train.index, 'close'])\n",
    "\n",
    "# test data sample weights\n",
    "return_based_sample_weights_test = get_weights_by_return(events.loc[X_test.index], dollar_bars.loc[X_test.index, 'close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Return based sample weights')\n",
    "return_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for cross validation\n",
    "parameters = {'max_depth':[3, 5, 7, 9],\n",
    "              'n_estimators':[10, 50, 100, 250, 500],\n",
    "              'C':[100,\n",
    "                   1000],\n",
    "              'gamma':[0.001,\n",
    "                       0.0001], \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the Purged K Fold splits needed for cross validation\n",
    "n_splits=4\n",
    "cv_gen_purged = PurgedKFold(n_splits=n_splits, samples_info_sets=events.loc[X_train.index].t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models we want to test\n",
    "selected_models = ['standard_bagging', # with base clf as Decision Tree Classifier\n",
    "                   'random_forest', # Random Forest without bagging\n",
    "                   'sequential_bootstrapping', # bagging using sequential bootstrapping method with base clf as Decision Tree Classifier\n",
    "                   'SVC', # standard SVC\n",
    "                   'seq_boot_SVC' # bagging using sequential bootstrapping method with base clf as Decision Tree Classifier SVC\n",
    "                   # RNN\n",
    "                   # BERT\n",
    "                   ]\n",
    "\n",
    "# setup metrics df\n",
    "model_metrics = pd.DataFrame(columns = ['type', 'top_model', 'max_cross_val_score', 'max_cross_val_score_recall', 'max_cross_val_score_precision', 'max_cross_val_score_accuracy','run_time'])\n",
    "\n",
    "# let's find out the best parameters and the best model!\n",
    "for clf in selected_models:\n",
    "    best_params = cv.perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', parameters, events, dollar_bars, type=clf, sample_weight=return_based_sample_weights.values)\n",
    "    model_metrics = model_metrics.append(best_params, ignore_index = True)  \n",
    "    print('Completed {}'.format(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best f1 score\n",
    "top_model = model_metrics.sort_values(['max_cross_val_score_recall']).tail(1)['top_model'].squeeze()\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_SVC = model_metrics[model_metrics['type'] == 'seq_boot_SVC']['top_model'].squeeze()\n",
    "best_SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Sequential Bootstrapped SVC classfier as the base model outperforms tree-based classifiers. Sequential Bootstrapping helps avoid overfitting by lowering down the probability of resampling repeated data during the bagging process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC classifier does not provide a direct wat to obtain feature importances. However, we can use the coefficients of the hyperplane that seperates the classes to estimate the importance of each feature. The magnitude of the coefficient corresponds to the importance of the corresponding feature in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "# Get coefficients of hyperplane\n",
    "coef = best_SVC.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients of hyperplane\n",
    "coef = top_model.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Model Metrics\n",
    "\n",
    "#### The test data is a bit too small to evaluate the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_SVC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "SVC_ROC = RocCurveDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the dataframe\n",
    "results_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred}, index=X_test.index)\n",
    "X_test_with_results = pd.concat([X_test, results_df], axis=1)\n",
    "merged_df = pd.merge(X_test_with_results, clean_labels, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade returns of predicted trades \n",
    "transaction_cost = 0.01\n",
    "slippage = 0.005\n",
    "\n",
    "merged_df['y_pred_trade_ret'] = 0\n",
    "merged_df.loc[merged_df['y_pred'] == 1, 'y_pred_trade_ret'] = merged_df.loc[merged_df['y_pred'] == 1, 'ret'] - (transaction_cost + slippage)\n",
    "merged_df_sorted = merged_df.sort_values('t1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative returns\n",
    "merged_df_sorted['cumulative_returns'] = (merged_df_sorted['y_pred_trade_ret'] + 1).cumprod() - 1\n",
    "cum_rtn_plot = merged_df_sorted.groupby('t1')['cumulative_returns', 'y_pred_trade_ret'].last()\n",
    "cum_rtn_plot = pd.DataFrame(cum_rtn_plot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(cum_rtn_plot.index, cum_rtn_plot.cumulative_returns)\n",
    "\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.hlines(y=0, xmin=None, xmax=None, colors='r', linestyles='--')\n",
    "plt.title('Backtest - Cumulative Returns {}'.format(str(round(cum_rtn_plot.iloc[-1].cumulative_returns*100,2))+'%'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpe Ratio (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ycharts.com/indicators/3_year_treasury_rate Long Term Average Rist Free Rate as at Mar 24, 2023\n",
    "daily_risk_free_rate = 0.036 / 252  # https://ycharts.com/indicators/3_year_treasury_rate Long Term Average Rist Free Rate as at Mar 24, 2023\n",
    "\n",
    "# calculate daily returns for each trading day\n",
    "trading_days = pd.DataFrame(pd.bdate_range(start=merged_df.index.min(), end=cum_rtn_plot.index.max(), freq='B'), columns=['trading_days'])\n",
    "trading_days.index =trading_days['trading_days']\n",
    "sharpe_df = pd.merge(trading_days, cum_rtn_plot['cumulative_returns'], left_index=True, right_index=True, how='left').fillna(method='ffill')\n",
    "sharpe_df['cumulative_returns'] = sharpe_df['cumulative_returns'].fillna(0)\n",
    "sharpe_df['daily_returns'] = sharpe_df['cumulative_returns'].diff().fillna(0)\n",
    "sharpe_df['day'] = 1\n",
    "sharpe_df['trading_days'] = sharpe_df['day'].cumsum()\n",
    "sharpe_df['rolling_std_dev'] =sharpe_df['daily_returns'].expanding().std(ddof=0)\n",
    "sharpe_df['cumulative_sharpe'] = ((sharpe_df['cumulative_returns']/sharpe_df['trading_days'])-daily_risk_free_rate) / sharpe_df['rolling_std_dev'] * np.sqrt(252)\n",
    "sharpe_df = sharpe_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average daily return and standard deviation of daily returns\n",
    "avg_daily_return = np.mean(sharpe_df.daily_returns)\n",
    "std_daily_return = np.std(sharpe_df.daily_returns)\n",
    "\n",
    "# Calculate the Sharpe ratio\n",
    "daily_risk_free_rate = 0.036 / 252  \n",
    "sharpe_ratio = round((avg_daily_return - daily_risk_free_rate) / std_daily_return * np.sqrt(252), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sharpe_df.index, sharpe_df['cumulative_sharpe'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.title('Backtest - Final Sharpe Ratio {}'.format(sharpe_ratio))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
