{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "from psycopg.rows import dict_row\n",
    "import getdata as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy.trendlabeling as tlb\n",
    "import afml.filters.filters as flt \n",
    "import afml.labeling.triplebarrier as tbar\n",
    "import afml.util.volatility as vol\n",
    "import features.bars as bars  \n",
    "import features.marketindicators as mkt\n",
    "from afml.sample_weights.attribution import get_weights_by_return\n",
    "from afml.cross_validation.cross_validation import PurgedKFold\n",
    "import crossvalidation as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "syntax error at end of input\nLINE 3:                 AND AM.close >= $1 AND AM.close <= $2 AND \n                                                                  ^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Init\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pgConnStr \u001b[39m=\u001b[39m gd\u001b[39m.\u001b[39mpgDictToConn(config\u001b[39m.\u001b[39mpgSecrets)\n\u001b[1;32m----> 4\u001b[0m tickerlst \u001b[39m=\u001b[39m gd\u001b[39m.\u001b[39;49mgetFilteredTickerList(lowest_price\u001b[39m=\u001b[39;49m\u001b[39m10.0\u001b[39;49m, highest_price\u001b[39m=\u001b[39;49m\u001b[39m20.0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\capstone_git\\MADS_Capstone\\getdata.py:101\u001b[0m, in \u001b[0;36mgetFilteredTickerList\u001b[1;34m(lowest_price, highest_price)\u001b[0m\n\u001b[0;32m     97\u001b[0m stmt \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\u001b[39mSELECT AM.symbol FROM alpaca_minute AS AM, last_tranx AS LT\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[39m    WHERE AM.symbol = LT.symbol AND AM.datetime = LT.latest\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[39m    AND AM.close >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m AND AM.close <= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m AND \u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m    100\u001b[0m data \u001b[39m=\u001b[39m (lowest_price, highest_price)\n\u001b[1;32m--> 101\u001b[0m result \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39;49mexecute(stmt, data)\u001b[39m.\u001b[39mfetchall()\n\u001b[0;32m    102\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m [row[\u001b[39m'\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\psycopg\\cursor.py:723\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, query, params, prepare, binary)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conn\u001b[39m.\u001b[39mwait(\n\u001b[0;32m    720\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_gen(query, params, prepare\u001b[39m=\u001b[39mprepare, binary\u001b[39m=\u001b[39mbinary)\n\u001b[0;32m    721\u001b[0m         )\n\u001b[0;32m    722\u001b[0m \u001b[39mexcept\u001b[39;00m e\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 723\u001b[0m     \u001b[39mraise\u001b[39;00m ex\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    724\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mSyntaxError\u001b[0m: syntax error at end of input\nLINE 3:                 AND AM.close >= $1 AND AM.close <= $2 AND \n                                                                  ^"
     ]
    }
   ],
   "source": [
    "#Init\n",
    "pgConnStr = gd.pgDictToConn(config.pgSecrets)\n",
    "\n",
    "tickerlst = gd.getFilteredTickerList(lowest_price=10.0, highest_price=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pick= list(np.random.choice(tickers, 10 if len(tickers) > 10 else  len(tickers), replace=False))\n",
    "random_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[39m=\u001b[39m gd\u001b[39m.\u001b[39mgetTicker(\u001b[39m'\u001b[39m\u001b[39mSTBA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df\u001b[39m.\u001b[39mdatetime)\n\u001b[1;32m----> 4\u001b[0m index_SPY \u001b[39m=\u001b[39m gd\u001b[39m.\u001b[39;49mgetTicker_Daily(\u001b[39m'\u001b[39;49m\u001b[39mSPY\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m index_SPY\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(index_SPY\u001b[39m.\u001b[39mdatetime)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\capstone_git\\MADS_Capstone\\getdata.py:74\u001b[0m, in \u001b[0;36mgetTicker_Daily\u001b[1;34m(symbol)\u001b[0m\n\u001b[0;32m     72\u001b[0m             cols \u001b[39m=\u001b[39m [col[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m desc]\n\u001b[0;32m     73\u001b[0m             conn\u001b[39m.\u001b[39mcommit()\n\u001b[1;32m---> 74\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mresult, columns\u001b[39m=\u001b[39mcols)\n\u001b[0;32m     75\u001b[0m \u001b[39m#     df['datetime'] = df['datetime'].dt.tz_convert('America/New_York')\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mtz_convert(\u001b[39m'\u001b[39m\u001b[39mAmerica/New_York\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "df = gd.getTicker('STBA')\n",
    "df.index = pd.to_datetime(df.datetime)\n",
    "\n",
    "index_SPY = gd.getTicker_Daily('SPY')\n",
    "index_SPY.index = pd.to_datetime(index_SPY.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling as Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the input data\n",
    "trades = df[['datetime', 'close', 'vol']].to_numpy()\n",
    "\n",
    "# define the dollar value to sample the data - we want to resample about 50 bars per day based on the average of last 10 days' dollar volume\n",
    "dollar_vol = (df['vol']*df['close']).resample('D').sum()[:-10].mean()/50.0\n",
    "\n",
    "# generate the dollar bars\n",
    "dollar_bars = bars.generate_dollarbars(trades, frequency=dollar_vol) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.plot(dollar_bars, # the dataframe containing the OHLC (Open, High, Low and Close) data\n",
    "         type='candle', # use candlesticks \n",
    "         volume=True, # also show the volume\n",
    "         mav=(9,20,200), # use three different moving averages\n",
    "         figratio=(3,1), # set the ratio of the figure\n",
    "         style='yahoo',  # choose the yahoo style\n",
    "         title='Dollar Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then do the same for SPY index as well for calculating relative strength as feature in Model 2 \n",
    "trades_SPY = index_SPY[['datetime', 'close', 'vol']].to_numpy()\n",
    "dollar_vol_SPY = (index_SPY['vol']*index_SPY['close']).resample('D').sum()[:10].mean()/50.0\n",
    "dollar_bars_SPY = bars.generate_dollarbars(trades_SPY, frequency=dollar_vol_SPY) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Trend Scanning to label as Uptrend or Downtrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get close values of dollar bars\n",
    "time_series = dollar_bars.close.to_numpy()\n",
    "\n",
    "# set window size to calculate best slope\n",
    "window_size_max= 7\n",
    "\n",
    "# get trend scanning labels\n",
    "label_output = pd.DataFrame(tlb.get_trend_scanning_labels(time_series=time_series, \n",
    "                                             window_size_max=window_size_max, \n",
    "                                             threshold=0.0,\n",
    "                                             opp_sign_ct=3,\n",
    "                                             side='both'), \n",
    "                            index= dollar_bars.index[window_size_max-1:])\n",
    "\n",
    "# put label results back into dollar_bars Dataframe \n",
    "dollar_bars = dollar_bars.join(label_output, how='outer')\n",
    "\n",
    "# remove look-ahead bias\n",
    "dollar_bars['label'] = dollar_bars['label'].shift(1) \n",
    "dollar_bars['slope'] = dollar_bars['slope'].shift(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of uptrends vs # of downtrends\n",
    "print(dollar_bars['label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Trade Triggering Events based on Volitility Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dollar bar close data\n",
    "close = dollar_bars.close.copy()\n",
    "\n",
    "# get Daily Volatility as 50 dollar bars exponentially weighted moving std dev\n",
    "dailyVolatility = vol.getDailyVol(close, span=50)\n",
    "\n",
    "# apply cusum filter to identify events as cumulative log return passed daily volatility threshold\n",
    "tEvents = flt.cusum_filter(close, threshold=dailyVolatility.mean()*0.5, signal=None)\n",
    "\n",
    "# Define vertical barrier - this is subjective, we will close out the position if after a day\n",
    "num_days = 1\n",
    "\n",
    "t1 = tbar.add_vertical_barrier(tEvents, close, num_days=num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show volitility over time\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "dailyVolatility.plot(ax=ax)\n",
    "ax.axhline(dailyVolatility.mean(),ls='--',color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show distribution of volitility\n",
    "ax = dailyVolatility.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get side labels from trend following method for inputting into the trend following method\n",
    "side_labels = []\n",
    "\n",
    "for dt in dollar_bars.index:\n",
    "    side_labels.append(dollar_bars.loc[dt]['label'])\n",
    "\n",
    "side_labels = pd.Series(side_labels, index=dollar_bars.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trade or Not Trade labels using Triple Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upper and lower horizontal barriers\n",
    "\n",
    "# set profit and stop loss ratio, 2:1 is the common trader's ratio\n",
    "ptsl = [1.5,1]\n",
    "\n",
    "# set minimum desired return\n",
    "# requires at least 1.5% percent return fro transaction and slippage\n",
    "minRet = 0.015 \n",
    "\n",
    "# Run in single-threaded mode on Windows\n",
    "import platform, os\n",
    "if platform.system() == \"Windows\":\n",
    "    cpus = 1\n",
    "else:\n",
    "    cpus = os.cpu_count() - 1\n",
    "    \n",
    "events = tbar.get_events(dollar_bars.close, \n",
    "                         t_events=tEvents, \n",
    "                         pt_sl=ptsl, \n",
    "                         target=dailyVolatility, \n",
    "                         min_ret=minRet, \n",
    "                         num_threads=cpus, \n",
    "                         vertical_barrier_times=t1,\n",
    "                         side_prediction=side_labels).dropna()\n",
    "\n",
    "# get the triple barrier labels 1 means above minimum return, 0 otherwise.\n",
    "labels = tbar.get_bins(triple_barrier_events = events, close=close)\n",
    "\n",
    "# Drop underpopulated labels\n",
    "clean_labels  = tbar.drop_labels(labels)\n",
    "print(clean_labels.bin.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is so imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Model 1 without features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Trend Labeling method, assuming we close out our position within 24 hours and we require a 1.5:1 profit ratio. If we bet on every single uptrend or downtrend signal, our accuracy is only at 20 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(clean_labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features to the dataset for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "dollar_bars['log_ret'] = np.log(dollar_bars['close']).diff().shift(1)\n",
    "\n",
    "# Momentum\n",
    "dollar_bars['mom1'] = dollar_bars['close'].pct_change(periods=1).shift(1)\n",
    "dollar_bars['mom2'] = dollar_bars['close'].pct_change(periods=2).shift(1)\n",
    "dollar_bars['mom3'] = dollar_bars['close'].pct_change(periods=3).shift(1)\n",
    "dollar_bars['mom4'] = dollar_bars['close'].pct_change(periods=4).shift(1)\n",
    "dollar_bars['mom5'] = dollar_bars['close'].pct_change(periods=5).shift(1)\n",
    "\n",
    "# Volatility\n",
    "dollar_bars['volatility_50'] = dollar_bars['log_ret'].rolling(window=50, min_periods=50, center=False).std().shift(1)\n",
    "dollar_bars['volatility_31'] = dollar_bars['log_ret'].rolling(window=31, min_periods=31, center=False).std().shift(1)\n",
    "dollar_bars['volatility_15'] = dollar_bars['log_ret'].rolling(window=15, min_periods=15, center=False).std().shift(1)\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "dollar_bars['autocorr_1'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False).shift(1)\n",
    "dollar_bars['autocorr_2'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=2), raw=False).shift(1)\n",
    "dollar_bars['autocorr_3'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False).shift(1)\n",
    "dollar_bars['autocorr_4'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=4), raw=False).shift(1)\n",
    "dollar_bars['autocorr_5'] = dollar_bars['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=5), raw=False).shift(1)\n",
    "\n",
    "# Get the various log -t returns\n",
    "dollar_bars['log_t1'] = dollar_bars['log_ret'].shift(1).shift(1)\n",
    "dollar_bars['log_t2'] = dollar_bars['log_ret'].shift(2).shift(1)\n",
    "dollar_bars['log_t3'] = dollar_bars['log_ret'].shift(3).shift(1)\n",
    "dollar_bars['log_t4'] = dollar_bars['log_ret'].shift(4).shift(1)\n",
    "dollar_bars['log_t5'] = dollar_bars['log_ret'].shift(5).shift(1)\n",
    "\n",
    "# relative strength SPY at various -t\n",
    "dollar_bars['rs_SPY_t1'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(1)\n",
    "dollar_bars['rs_SPY_t2'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(2)\n",
    "dollar_bars['rs_SPY_t3'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(3)\n",
    "dollar_bars['rs_SPY_t4'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(4)\n",
    "dollar_bars['rs_SPY_t5'] = mkt.get_relative_strength(dollar_bars.close, dollar_bars_SPY.close).shift(5)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Model Fitting\n",
    "\n",
    "#### Use features: volatility, serial correlation, relative strength to S&P, and original Model 1 slope and trend label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_bars = dollar_bars.join(clean_labels['bin']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dollar_bars.iloc[:, :-1]\n",
    "y = dollar_bars.iloc[:, -1]\n",
    "\n",
    "col = ['slope', 'label', 'log_ret',\n",
    "       'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'volatility_50',\n",
    "       'volatility_31', 'volatility_15', 'autocorr_1', 'autocorr_2',\n",
    "       'autocorr_3', 'autocorr_4', 'autocorr_5', 'log_t1', 'log_t2', 'log_t3',\n",
    "       'log_t4', 'log_t5', 'rs_SPY_t1', 'rs_SPY_t2', 'rs_SPY_t3', 'rs_SPY_t4',\n",
    "       'rs_SPY_t5']\n",
    "\n",
    "X = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to Label Concurrency, we will apply custom sample weights to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data sample weights\n",
    "return_based_sample_weights = get_weights_by_return(events.loc[X_train.index], dollar_bars.loc[X_train.index, 'close'])\n",
    "\n",
    "# test data sample weights\n",
    "return_based_sample_weights_test = get_weights_by_return(events.loc[X_test.index], dollar_bars.loc[X_test.index, 'close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Return based sample weights')\n",
    "return_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for cross validation\n",
    "parameters = {'max_depth':[3, 5, 7, 9],\n",
    "              'n_estimators':[10, 50, 100, 250, 500],\n",
    "              'C':[100,\n",
    "                   1000],\n",
    "              'gamma':[0.001,\n",
    "                       0.0001], \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the Purged K Fold splits needed for cross validation\n",
    "n_splits=4\n",
    "cv_gen_purged = PurgedKFold(n_splits=n_splits, samples_info_sets=events.loc[X_train.index].t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models we want to test\n",
    "selected_models = ['standard_bagging', # with base clf as Decision Tree Classifier\n",
    "                   'random_forest', # Random Forest without bagging\n",
    "                   'sequential_bootstrapping', # bagging using sequential bootstrapping method with base clf as Decision Tree Classifier\n",
    "                   'SVC', # standard SVC\n",
    "                   'seq_boot_SVC' # bagging using sequential bootstrapping method with base clf as Decision Tree Classifier SVC\n",
    "                   # RNN\n",
    "                   # BERT\n",
    "                   ]\n",
    "\n",
    "# setup metrics df\n",
    "model_metrics = pd.DataFrame(columns = ['type', 'top_model', 'max_cross_val_score', 'max_cross_val_score_recall', 'max_cross_val_score_precision', 'max_cross_val_score_accuracy','run_time'])\n",
    "\n",
    "# let's find out the best parameters and the best model!\n",
    "for clf in selected_models:\n",
    "    best_params = cv.perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', parameters, events, dollar_bars, type=clf, sample_weight=return_based_sample_weights.values)\n",
    "    model_metrics = model_metrics.append(best_params, ignore_index = True)  \n",
    "    print('Completed {}'.format(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best f1 score\n",
    "top_model = model_metrics.sort_values(['max_cross_val_score_recall']).tail(1)['top_model'].squeeze()\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_SVC = model_metrics[model_metrics['type'] == 'seq_boot_SVC']['top_model'].squeeze()\n",
    "best_SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Sequential Bootstrapped SVC classfier as the base model outperforms tree-based classifiers. Sequential Bootstrapping helps avoid overfitting by lowering down the probability of resampling repeated data during the bagging process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC classifier does not provide a direct wat to obtain feature importances. However, we can use the coefficients of the hyperplane that seperates the classes to estimate the importance of each feature. The magnitude of the coefficient corresponds to the importance of the corresponding feature in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "# Get coefficients of hyperplane\n",
    "coef = best_SVC.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients of hyperplane\n",
    "coef = top_model.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Model Metrics\n",
    "\n",
    "#### The test data is a bit too small to evaluate the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_SVC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "SVC_ROC = RocCurveDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the dataframe\n",
    "results_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred}, index=X_test.index)\n",
    "X_test_with_results = pd.concat([X_test, results_df], axis=1)\n",
    "merged_df = pd.merge(X_test_with_results, clean_labels, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade returns of predicted trades \n",
    "transaction_cost = 0.01\n",
    "slippage = 0.005\n",
    "\n",
    "merged_df['y_pred_trade_ret'] = 0\n",
    "merged_df.loc[merged_df['y_pred'] == 1, 'y_pred_trade_ret'] = merged_df.loc[merged_df['y_pred'] == 1, 'ret'] - (transaction_cost + slippage)\n",
    "merged_df_sorted = merged_df.sort_values('t1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative returns\n",
    "merged_df_sorted['cumulative_returns'] = (merged_df_sorted['y_pred_trade_ret'] + 1).cumprod() - 1\n",
    "cum_rtn_plot = merged_df_sorted.groupby('t1')['cumulative_returns', 'y_pred_trade_ret'].last()\n",
    "cum_rtn_plot = pd.DataFrame(cum_rtn_plot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(cum_rtn_plot.index, cum_rtn_plot.cumulative_returns)\n",
    "\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.hlines(y=0, xmin=None, xmax=None, colors='r', linestyles='--')\n",
    "plt.title('Backtest - Cumulative Returns {}'.format(str(round(cum_rtn_plot.iloc[-1].cumulative_returns*100,2))+'%'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpe Ratio (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ycharts.com/indicators/3_year_treasury_rate Long Term Average Rist Free Rate as at Mar 24, 2023\n",
    "daily_risk_free_rate = 0.036 / 252  # https://ycharts.com/indicators/3_year_treasury_rate Long Term Average Rist Free Rate as at Mar 24, 2023\n",
    "\n",
    "# calculate daily returns for each trading day\n",
    "trading_days = pd.DataFrame(pd.bdate_range(start=merged_df.index.min(), end=cum_rtn_plot.index.max(), freq='B'), columns=['trading_days'])\n",
    "trading_days.index =trading_days['trading_days']\n",
    "sharpe_df = pd.merge(trading_days, cum_rtn_plot['cumulative_returns'], left_index=True, right_index=True, how='left').fillna(method='ffill')\n",
    "sharpe_df['cumulative_returns'] = sharpe_df['cumulative_returns'].fillna(0)\n",
    "sharpe_df['daily_returns'] = sharpe_df['cumulative_returns'].diff().fillna(0)\n",
    "sharpe_df['day'] = 1\n",
    "sharpe_df['trading_days'] = sharpe_df['day'].cumsum()\n",
    "sharpe_df['rolling_std_dev'] =sharpe_df['daily_returns'].expanding().std(ddof=0)\n",
    "sharpe_df['cumulative_sharpe'] = ((sharpe_df['cumulative_returns']/sharpe_df['trading_days'])-daily_risk_free_rate) / sharpe_df['rolling_std_dev'] * np.sqrt(252)\n",
    "sharpe_df = sharpe_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average daily return and standard deviation of daily returns\n",
    "avg_daily_return = np.mean(sharpe_df.daily_returns)\n",
    "std_daily_return = np.std(sharpe_df.daily_returns)\n",
    "\n",
    "# Calculate the Sharpe ratio\n",
    "daily_risk_free_rate = 0.036 / 252  \n",
    "sharpe_ratio = round((avg_daily_return - daily_risk_free_rate) / std_daily_return * np.sqrt(252), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sharpe_df.index, sharpe_df['cumulative_sharpe'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.title('Backtest - Final Sharpe Ratio {}'.format(sharpe_ratio))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
