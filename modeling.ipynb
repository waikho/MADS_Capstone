{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy.trendlabeling as tlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getdata as gd\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "\n",
    "import afml.filters.filters as flt \n",
    "import afml.labeling.triplebarrier as tbar\n",
    "import afml.util.volatility as vol\n",
    "import features.bars as bars  \n",
    "import features.marketindicators as mkt\n",
    "import afml.features.fracdiff as fdiff\n",
    "from afml.ensemble.sb_bagging import SequentiallyBootstrappedBaggingClassifier\n",
    "from afml.sample_weights.attribution import get_weights_by_return, get_weights_by_time_decay\n",
    "from afml.feature_importance.importance import mean_decrease_impurity, mean_decrease_accuracy, single_feature_importance, plot_feature_importance\n",
    "from afml.cross_validation.cross_validation import PurgedKFold, ml_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yf = gd.get_yf_data(tickers= \"SPY COMP ALGM\", \n",
    "#                     period='60d',   \n",
    "#                     interval='5m'\n",
    "# )\n",
    "\n",
    "# df = df_yf[df_yf['Ticker'] == 'ALGM']\n",
    "# index_SPY = df_yf[df_yf['Ticker'] == 'SPY']\n",
    "# index_COMP = df_yf[df_yf['Ticker'] == 'COMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete HAL 2021\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete HAL 2020\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete HAL 2019\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete HAL 2018\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete HAL 2017\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete SPY 2021\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete SPY 2020\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete SPY 2019\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete SPY 2018\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Complete SPY 2017\n"
     ]
    }
   ],
   "source": [
    "# download dataframe\n",
    "\n",
    "yf.pdr_override()\n",
    "\n",
    "def get_yf_daily(ticker, startdate, enddate, years):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(years):\n",
    "        start = startdate.replace(startdate.year-i).strftime('%Y-%m-%d')\n",
    "        end = enddate.replace(enddate.year-i).strftime('%Y-%m-%d')\n",
    "        thisdf = pdr.get_data_yahoo(ticker, start=start, end=end)\n",
    "        df= pd.concat([df, thisdf], axis= 0)\n",
    "        print('Complete {} {}'.format(ticker, enddate.year-i))\n",
    "    return df\n",
    "\n",
    "years = 5\n",
    "startdate= datetime.date(2021, 1, 1)\n",
    "enddate = datetime.date(2021, 12, 31)\n",
    "\n",
    "df = get_yf_daily('HAL', startdate, enddate, years)\n",
    "index_SPY = get_yf_daily('SPY', startdate, enddate, years)\n",
    "#index_COMP = get_yf_daily('COMP', startdate, enddate, years)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Trend Scanning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Adj Close to numpy\n",
    "time_series = df['Adj Close'].to_numpy()\n",
    "window_size_max= 7\n",
    "\n",
    "# get trend scanning labels\n",
    "label_output = tlb.get_trend_scanning_labels(time_series=time_series, \n",
    "                                             window_size_max=window_size_max, \n",
    "                                             threshold=0.0,\n",
    "                                             opp_sign_ct=3,\n",
    "                                             side='up')\n",
    "\n",
    "# drop last rolling window size -1 rows\n",
    "n = window_size_max-1\n",
    "#df.drop(df.tail(n).index, inplace = True)\n",
    "df = df.iloc[:-n]\n",
    "\n",
    "# append the slope and labels to the df\n",
    "df['slope'] = label_output['slope']\n",
    "df['label'] = label_output['label']\n",
    "# df['isEvent'] = label_output['isEvent']\n",
    "# isEvent = df[df['isEvent']==1].index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Simple moving average cross over strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to cite here ...\n",
    "\n",
    "# compute moving averages\n",
    "fast_window = 20\n",
    "slow_window = 50\n",
    "\n",
    "df['fast_mavg'] = df['Adj Close'].rolling(window=fast_window, min_periods=fast_window, center=False).mean()\n",
    "df['slow_mavg'] = df['Adj Close'].rolling(window=slow_window, min_periods=slow_window, center=False).mean()\n",
    "df.head()\n",
    "\n",
    "# Compute sides\n",
    "df['side'] = np.nan\n",
    "\n",
    "long_signals = df['fast_mavg'] >= df['slow_mavg'] \n",
    "short_signals = df['fast_mavg'] < df['slow_mavg'] \n",
    "df.loc[long_signals, 'side'] = 1\n",
    "df.loc[short_signals, 'side'] = -1\n",
    "\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "df['side'] = df['side'].shift(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "# Drop the NaN values from our data set\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    601\n",
      " 1    598\n",
      "Name: label, dtype: int64\n",
      "-1.0    657\n",
      " 1.0    542\n",
      "Name: side, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n",
    "print(df['side'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the input data\n",
    "_df = df.reset_index()\n",
    "trades = _df[['Date', 'Adj Close', 'Volume']].to_numpy()\n",
    "\n",
    "# define the dollar value to sample the data\n",
    "frequency = _df.Volume.mean()*20\n",
    "#frequency = df['Volume'].resample('D').sum().mean()/10.0\n",
    "\n",
    "# generate the dollar bars\n",
    "dollar_bars = bars.generate_dollarbars(trades, frequency=frequency) \n",
    "\n",
    "# define closing price\n",
    "close = dollar_bars.close.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Events using CUSUM Filter\n",
    "\n",
    "We will then predict what will happen if the event is triggered, based on the 'side' signal from the Trend Following Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Daily Volatility\n",
    "dailyVolatility = vol.getDailyVol(close, span=50)\n",
    "\n",
    "# apply cusum filter to identify events as cumulative log return passed threshold\n",
    "#tEvents = flt.getTEvents(close, h=dailyVolatility.mean()*0.5)\n",
    "tEvents = flt.cusum_filter(close, threshold=dailyVolatility.mean()*0.5, signal=None)\n",
    "\n",
    "# Define vertical barrier - subjective judgment\n",
    "num_days = 10\n",
    "\n",
    "t1 = tbar.add_vertical_barrier(tEvents, close, num_days=num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get side labels from trend following method\n",
    "side_labels = []\n",
    "\n",
    "for dt in dollar_bars.index:\n",
    "    side_labels.append(df.loc[dt]['label'])\n",
    "\n",
    "side_labels = pd.Series(side_labels, index=dollar_bars.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trade or Not Trade labels using Triple Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "index must be monotonic increasing or decreasing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m\n\u001b[0;32m     13\u001b[0m     cpus \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mcpu_count() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     15\u001b[0m events \u001b[39m=\u001b[39m tbar\u001b[39m.\u001b[39mget_events(dollar_bars\u001b[39m.\u001b[39mclose, \n\u001b[0;32m     16\u001b[0m                          t_events\u001b[39m=\u001b[39mtEvents, \n\u001b[0;32m     17\u001b[0m                          pt_sl\u001b[39m=\u001b[39mptsl, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m                          vertical_barrier_times\u001b[39m=\u001b[39mt1,\n\u001b[0;32m     22\u001b[0m                          side_prediction\u001b[39m=\u001b[39mside_labels)\u001b[39m.\u001b[39mdropna()\n\u001b[1;32m---> 24\u001b[0m labels \u001b[39m=\u001b[39m tbar\u001b[39m.\u001b[39;49mget_bins(triple_barrier_events \u001b[39m=\u001b[39;49m events, close\u001b[39m=\u001b[39;49mclose)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Drop underpopulated labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m clean_labels  \u001b[39m=\u001b[39m tbar\u001b[39m.\u001b[39mdrop_labels(labels)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\capstone_git\\MADS_Capstone\\afml\\labeling\\triplebarrier.py:234\u001b[0m, in \u001b[0;36mget_bins\u001b[1;34m(triple_barrier_events, close)\u001b[0m\n\u001b[0;32m    232\u001b[0m events_ \u001b[39m=\u001b[39m triple_barrier_events\u001b[39m.\u001b[39mdropna(subset\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mt1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    233\u001b[0m all_dates \u001b[39m=\u001b[39m events_\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39munion(other\u001b[39m=\u001b[39mevents_[\u001b[39m'\u001b[39m\u001b[39mt1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39marray)\u001b[39m.\u001b[39mdrop_duplicates()\n\u001b[1;32m--> 234\u001b[0m prices \u001b[39m=\u001b[39m close\u001b[39m.\u001b[39;49mreindex(all_dates, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbfill\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    236\u001b[0m \u001b[39m# 2) Create out DataFrame\u001b[39;00m\n\u001b[0;32m    237\u001b[0m out_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mevents_\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\series.py:5094\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5090\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   5091\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m passed as both positional and keyword argument\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5092\u001b[0m         )\n\u001b[0;32m   5093\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index})\n\u001b[1;32m-> 5094\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\generic.py:5289\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   5288\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_axes(\n\u001b[0;32m   5290\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[0;32m   5291\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\generic.py:5304\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5301\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   5303\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5304\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49mreindex(\n\u001b[0;32m   5305\u001b[0m     labels, level\u001b[39m=\u001b[39;49mlevel, limit\u001b[39m=\u001b[39;49mlimit, tolerance\u001b[39m=\u001b[39;49mtolerance, method\u001b[39m=\u001b[39;49mmethod\n\u001b[0;32m   5306\u001b[0m )\n\u001b[0;32m   5308\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m   5309\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   5310\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   5311\u001b[0m     fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[0;32m   5312\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   5313\u001b[0m     allow_dups\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   5314\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4412\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 4412\u001b[0m         indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(\n\u001b[0;32m   4413\u001b[0m             target, method\u001b[39m=\u001b[39;49mmethod, limit\u001b[39m=\u001b[39;49mlimit, tolerance\u001b[39m=\u001b[39;49mtolerance\n\u001b[0;32m   4414\u001b[0m         )\n\u001b[0;32m   4415\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi:\n\u001b[0;32m   4416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot handle a non-unique multi-index!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3973\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3968\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3969\u001b[0m     \u001b[39mreturn\u001b[39;00m this\u001b[39m.\u001b[39m_get_indexer(\n\u001b[0;32m   3970\u001b[0m         target, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance\n\u001b[0;32m   3971\u001b[0m     )\n\u001b[1;32m-> 3973\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indexer(target, method, limit, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3986\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3983\u001b[0m     tolerance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_tolerance(tolerance, target)\n\u001b[0;32m   3985\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbackfill\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m-> 3986\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_fill_indexer(target, method, limit, tolerance)\n\u001b[0;32m   3987\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3988\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_nearest_indexer(target, limit, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4101\u001b[0m, in \u001b[0;36mIndex._get_fill_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   4099\u001b[0m         indexer \u001b[39m=\u001b[39m libalgos\u001b[39m.\u001b[39mbackfill(own_values, target_values, limit\u001b[39m=\u001b[39mlimit)\n\u001b[0;32m   4100\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4101\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_fill_indexer_searchsorted(target, method, limit)\n\u001b[0;32m   4102\u001b[0m \u001b[39mif\u001b[39;00m tolerance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   4103\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_indexer_tolerance(target, indexer, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4125\u001b[0m, in \u001b[0;36mIndex._get_fill_indexer_searchsorted\u001b[1;34m(self, target, method, limit)\u001b[0m\n\u001b[0;32m   4123\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer(target)\n\u001b[0;32m   4124\u001b[0m nonexact \u001b[39m=\u001b[39m indexer \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 4125\u001b[0m indexer[nonexact] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_searchsorted_monotonic(target[nonexact], side)\n\u001b[0;32m   4126\u001b[0m \u001b[39mif\u001b[39;00m side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   4127\u001b[0m     \u001b[39m# searchsorted returns \"indices into a sorted array such that,\u001b[39;00m\n\u001b[0;32m   4128\u001b[0m     \u001b[39m# if the corresponding elements in v were inserted before the\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     \u001b[39m# indices, the order of a would be preserved\".\u001b[39;00m\n\u001b[0;32m   4130\u001b[0m     \u001b[39m# Thus, we need to subtract 1 to find values to the left.\u001b[39;00m\n\u001b[0;32m   4131\u001b[0m     indexer[nonexact] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6639\u001b[0m, in \u001b[0;36mIndex._searchsorted_monotonic\u001b[1;34m(self, label, side)\u001b[0m\n\u001b[0;32m   6634\u001b[0m     pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msearchsorted(\n\u001b[0;32m   6635\u001b[0m         label, side\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   6636\u001b[0m     )\n\u001b[0;32m   6637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m pos\n\u001b[1;32m-> 6639\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex must be monotonic increasing or decreasing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: index must be monotonic increasing or decreasing"
     ]
    }
   ],
   "source": [
    "# Define upper and lower horizontal barriers\n",
    "# set profit and stop loss ratio\n",
    "ptsl = [2,1]\n",
    "\n",
    "# select minRet\n",
    "minRet = 0.01 # requires at least 1 percent return\n",
    "\n",
    "# Run in single-threaded mode on Windows\n",
    "import platform, os\n",
    "if platform.system() == \"Windows\":\n",
    "    cpus = 1\n",
    "else:\n",
    "    cpus = os.cpu_count() - 1\n",
    "    \n",
    "events = tbar.get_events(dollar_bars.close, \n",
    "                         t_events=tEvents, \n",
    "                         pt_sl=ptsl, \n",
    "                         target=dailyVolatility, \n",
    "                         min_ret=minRet, \n",
    "                         num_threads=cpus, \n",
    "                         vertical_barrier_times=t1,\n",
    "                         side_prediction=side_labels).dropna()\n",
    "\n",
    "labels = tbar.get_bins(triple_barrier_events = events, close=close)\n",
    "\n",
    "# Drop underpopulated labels\n",
    "clean_labels  = tbar.drop_labels(labels)\n",
    "print(clean_labels.bin.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Secondary Model without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(clean_labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a Meta model\n",
    "\n",
    "Use features: volatility, serial correlation, relative strength to S&P and COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['Adj Close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['Adj Close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['Adj Close'].pct_change(periods=2)\n",
    "raw_data['mom3'] = raw_data['Adj Close'].pct_change(periods=3)\n",
    "raw_data['mom4'] = raw_data['Adj Close'].pct_change(periods=4)\n",
    "raw_data['mom5'] = raw_data['Adj Close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=3, center=False).std()\n",
    "raw_data['volatility_31'] = raw_data['log_ret'].rolling(window=31, min_periods=3, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=3, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=5), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t3'] = raw_data['log_ret'].shift(3)\n",
    "raw_data['log_t4'] = raw_data['log_ret'].shift(4)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re compute sides\n",
    "# raw_data['side'] = np.nan\n",
    "\n",
    "# long_signals = raw_data['fast_mavg'] >= raw_data['slow_mavg']\n",
    "# short_signals = raw_data['fast_mavg'] < raw_data['slow_mavg']\n",
    "\n",
    "# raw_data.loc[long_signals, 'side'] = 1\n",
    "# raw_data.loc[short_signals, 'side'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial correlation\n",
    "srl_corr = mkt.df_rolling_autocorr(mkt.returns(close), window=7).rename('srl_corr')\n",
    "\n",
    "# relative strength to SPY\n",
    "rs_SPY = mkt.get_relative_strength(df['Adj Close'], index_SPY['Adj Close']).shift(1).dropna()\n",
    "\n",
    "# relative strength to COMP\n",
    "#rs_COMP = mkt.get_relative_strength(df['Adj Close'], index_COMP['Adj Close']).shift(1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractional differentiated log dollar bar prices\n",
    "\n",
    "# cumulative sum of stock log-prices\n",
    "ticker_log_cumsum = np.log(dollar_bars.close).cumsum()\n",
    "\n",
    "# frac diff 1 time\n",
    "dfx1 = fdiff.frac_diff_ffd(ticker_log_cumsum.to_frame(), diff_amt=1).dropna()\n",
    "\n",
    "# apply cumsum filter\n",
    "dfx1_close = dfx1.close.copy()\n",
    "df_tEvents = flt.getTEvents(dfx1_close, h=dfx1.std().iat[0]*2)\n",
    "\n",
    "# fracDiff value feature\n",
    "frac_diff_feat = dfx1.loc[df_tEvents] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (pd.DataFrame()\n",
    "            .assign(vol=events.trgt)\n",
    "            #.assign(side=clean_labels.side)\n",
    "            #.assign(srl_corr=srl_corr)\n",
    "            .assign(rs_SPY=rs_SPY)\n",
    "            #.assign(rs_COMP=rs_COMP)\n",
    "            #.assign(frac_diff_feat=frac_diff_feat)\n",
    "            .drop_duplicates()\n",
    "            .dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "_X = raw_data.loc[clean_labels.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "_X.drop([#'Ticker',\n",
    "        'Adj Close', 'High', 'Low', 'Open', 'Volume', \n",
    "        #'fast_mavg', 'slow_mavg', 'side', # remove for MA crossover\n",
    "        #'slope', 'label', # remove for trend scanning\n",
    "        ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = _X.join(features).join(clean_labels['bin']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xy.iloc[:, :-1]\n",
    "y = Xy.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, shuffle=False, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data sample weights\n",
    "\n",
    "return_based_sample_weights = get_weights_by_return(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'])\n",
    "time_based_sample_weights = get_weights_by_time_decay(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'], decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Return based sample weights')\n",
    "return_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Time decaying sample weights')\n",
    "time_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring, sample_weight, scaler=StandardScaler()):\n",
    "    t0 = 0.0\n",
    "    t1 = 0.0\n",
    "\n",
    "    best_param_dict = {}\n",
    "    best_param_dict['type'] = type\n",
    "    best_param_dict['top_model'] = None\n",
    "    best_param_dict['max_cross_val_score'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_recall'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_precision'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_accuracy'] = -np.inf\n",
    "    best_param_dict['run_time'] = 0.0\n",
    "\n",
    "    col = X_train.columns.to_list()\n",
    "    idx = X_train.index\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=col, index=idx)\n",
    "\n",
    "    t0 = time.time()\n",
    "    temp_score_base, temp_recall, temp_precision, temp_accuracy = ml_cross_val_score(clf, X_train_scaled, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    if temp_score_base.mean() > best_param_dict['max_cross_val_score']:\n",
    "        best_param_dict['top_model'] = clf\n",
    "        best_param_dict['max_cross_val_score'] = temp_score_base.mean()\n",
    "        best_param_dict['max_cross_val_score_recall'] = temp_recall.mean()\n",
    "        best_param_dict['max_cross_val_score_precision'] = temp_precision.mean()\n",
    "        best_param_dict['max_cross_val_score_accuracy'] = temp_accuracy.mean()\n",
    "        best_param_dict['run_time'] = t1-t0    \n",
    "    \n",
    "    return best_param_dict\n",
    "\n",
    "def perform_grid_search(X_train, y_train, cv_gen, scoring, parameters, type='standard', sample_weight=None, RANDOM_STATE=42):\n",
    "    \"\"\"\n",
    "    Grid search using Purged CV without using sample weights in fit(). Returns top model and top score\n",
    "    \"\"\"\n",
    "\n",
    "    if type=='SVC' or type=='seq_boot_SVC':\n",
    "        for C in parameters['C']:\n",
    "            for gamma in parameters['gamma']:\n",
    "\n",
    "                clf_SVC = SVC(C=C,\n",
    "                                gamma=gamma,\n",
    "                                class_weight='balanced',\n",
    "                                kernel='linear',\n",
    "                                random_state=RANDOM_STATE)\n",
    "\n",
    "                if type =='SVC':\n",
    "                    clf = clf_SVC\n",
    "                elif type == 'seq_boot_SVC':\n",
    "                    clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, ## events\n",
    "                                                                price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], ## df\n",
    "                                                                estimator=clf_SVC, \n",
    "                                                                random_state=RANDOM_STATE, n_jobs=-1, oob_score=False,\n",
    "                                                                max_features=1.)\n",
    "\n",
    "                # get best param dict   \n",
    "                best_param_dict = get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "    else:    \n",
    "        for m_depth in parameters['max_depth']:\n",
    "            for n_est in parameters['n_estimators']:\n",
    "                clf_base = DecisionTreeClassifier(criterion='entropy', random_state=RANDOM_STATE, \n",
    "                                                max_depth=m_depth, class_weight='balanced')\n",
    "\n",
    "                if type == 'standard_bagging':\n",
    "                    clf = BaggingClassifier(n_estimators=n_est, \n",
    "                                            estimator=clf_base, \n",
    "                                            random_state=RANDOM_STATE, n_jobs=-1, \n",
    "                                            oob_score=False, max_features=1.)\n",
    "                elif type == 'random_forest':\n",
    "                    clf = RandomForestClassifier(n_estimators=n_est, \n",
    "                                                max_depth=m_depth, \n",
    "                                                random_state=RANDOM_STATE, \n",
    "                                                n_jobs=-1, \n",
    "                                                oob_score=False, \n",
    "                                                criterion='entropy',\n",
    "                                                class_weight='balanced_subsample', \n",
    "                                                max_features=1.)\n",
    "                elif type == 'sequential_bootstrapping':\n",
    "                    clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, ## events\n",
    "                                                                    price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], ## df\n",
    "                                                                    estimator=clf_base, \n",
    "                                                                    n_estimators=n_est, \n",
    "                                                                    random_state=RANDOM_STATE, \n",
    "                                                                    n_jobs=-1, \n",
    "                                                                    oob_score=False,\n",
    "                                                                    max_features=1.)\n",
    "                \n",
    "                # get best param dict   \n",
    "                best_param_dict = get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "\n",
    "    return best_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[2, 3, 4, 5, 7],\n",
    "              'n_estimators':[10, 25, 50, 100, 256, 512],\n",
    "              'C':[1,10,100,1000],\n",
    "              'gamma':[1,0.1,0.001,0.0001], \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=4\n",
    "\n",
    "cv_gen_standard = KFold(n_splits, random_state=RANDOM_STATE)\n",
    "cv_gen_purged = PurgedKFold(n_splits=n_splits, samples_info_sets=events.loc[X_train.index].t1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_models = ['standard_bagging', \n",
    "                   'random_forest', \n",
    "                   'sequential_bootstrapping', \n",
    "                   'SVC', \n",
    "                   'seq_boot_SVC'\n",
    "                   ]\n",
    "\n",
    "model_metrics = pd.DataFrame(columns = ['type', 'top_model', 'max_cross_val_score', 'max_cross_val_score_recall', 'max_cross_val_score_precision', 'max_cross_val_score_accuracy','run_time'])\n",
    "\n",
    "\n",
    "for clf in selected_models:\n",
    "    best_params = perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', parameters, type=clf, sample_weight=return_based_sample_weights.values)\n",
    "    model_metrics = model_metrics.append(best_params, ignore_index = True)  \n",
    "    print('Completed {}'.format(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = model_metrics.sort_values(['max_cross_val_score']).tail(1)['top_model'].squeeze()\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVC = model_metrics[model_metrics['type'] == 'SVC']['top_model'].squeeze()\n",
    "best_SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC classifier does not provide a direct wat to obtain feature importances. However, we can use the coefficients of the hyperplane that seperates the classes to estimate the importance of each feature. The magnitude of the coefficient corresponds to the importance of the corresponding feature in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients of hyperplane\n",
    "coef = best_SVC.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This codes below are for tress based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MDI, MDA, SFI feature importance\n",
    "# mdi_feat_imp = mean_decrease_impurity(top_model, X_train.columns)\n",
    "# mda_feat_imp = mean_decrease_accuracy(top_model, X_train, y_train, cv_gen_purged, scoring='f1', sample_weight=sw_train)\n",
    "# sfi_feat_imp = single_feature_importance(top_model, X_train, y_train, cv_gen_purged, scoring='f1', sample_weight=sw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(mdi_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(mda_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(sfi_feat_imp, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "#best_SVC_fitted = best_SVC.fit(X_train, y_train, sample_weight=return_based_sample_weights.values)\n",
    "#y_pred = best_SVC_fitted.predict(X_test)\n",
    "y_pred = best_SVC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "SVC_ROC = RocCurveDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = top_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(top_model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_estimator(top_model, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = X_test[['slope', 'label']]\n",
    "test_result_df['y_test'] = y_test\n",
    "test_result_df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df[test_result_df['label']==1].groupby('y_test').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "   \n",
    "   # Load the data\n",
    "data = pd.read_csv('your_data.csv')\n",
    "   \n",
    "   # Calculate the daily returns\n",
    "data['daily_returns'] = data['log_returns'].apply(lambda x: math.exp(x) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transaction cost and slippage parameters\n",
    "transaction_cost = 0.01  # 1% transaction cost\n",
    "slippage = 0.005  # 0.5% slippage\n",
    "\n",
    "# Calculate the trade returns\n",
    "data['trade_returns'] = data['daily_returns'] * data['trade'].shift(1) - abs(data['trade'].diff()) * (transaction_cost + slippage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative returns\n",
    "data['cumulative_returns'] = (data['trade_returns'] + 1).cumprod() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart of the cumulative returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "   # Visualize the results\n",
    "plt.plot(data['date'], data['cumulative_returns'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.title('Backtest Results')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
