{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy.trendlabeling as tlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getdata as gd\n",
    "import afml.filters.filters as flt \n",
    "import afml.labeling.triplebarrier as tbar\n",
    "import afml.util.volatility as vol\n",
    "import features.bars as bars  \n",
    "import features.marketindicators as mkt\n",
    "import afml.features.fracdiff as fdiff\n",
    "from afml.ensemble.sb_bagging import SequentiallyBootstrappedBaggingClassifier\n",
    "from afml.sample_weights.attribution import get_weights_by_return, get_weights_by_time_decay\n",
    "from afml.feature_importance.importance import mean_decrease_impurity, mean_decrease_accuracy, single_feature_importance, plot_feature_importance\n",
    "from afml.cross_validation.cross_validation import PurgedKFold, ml_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "df_yf = gd.get_yf_data(tickers= \"SPY COMP ALGM\", \n",
    "                    period='60d', #'1y',   \n",
    "                    interval='5m' #'1d'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_yf[df_yf['Ticker'] == 'ALGM']\n",
    "index_SPY = df_yf[df_yf['Ticker'] == 'SPY']\n",
    "index_COMP = df_yf[df_yf['Ticker'] == 'COMP']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Trend Scanning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Adj Close to numpy\n",
    "time_series = df['Adj Close'].to_numpy()\n",
    "window_size_max= 7\n",
    "\n",
    "# get trend scanning labels\n",
    "label_output = tlb.get_trend_scanning_labels(time_series=time_series, \n",
    "                                             window_size_max=window_size_max, \n",
    "                                             threshold=0.0,\n",
    "                                             opp_sign_ct=3,\n",
    "                                             side='up')\n",
    "\n",
    "# drop last rolling window size -1 rows\n",
    "n = window_size_max-1\n",
    "#df.drop(df.tail(n).index, inplace = True)\n",
    "df = df.iloc[:-n]\n",
    "\n",
    "# append the slope and labels to the df\n",
    "df['slope'] = label_output['slope']\n",
    "df['label'] = label_output['label']\n",
    "# df['isEvent'] = label_output['isEvent']\n",
    "# isEvent = df[df['isEvent']==1].index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Simple moving average cross over strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to cite here ...\n",
    "\n",
    "# compute moving averages\n",
    "fast_window = 20\n",
    "slow_window = 50\n",
    "\n",
    "df['fast_mavg'] = df['Adj Close'].rolling(window=fast_window, min_periods=fast_window, center=False).mean()\n",
    "df['slow_mavg'] = df['Adj Close'].rolling(window=slow_window, min_periods=slow_window, center=False).mean()\n",
    "df.head()\n",
    "\n",
    "# Compute sides\n",
    "df['side'] = np.nan\n",
    "\n",
    "long_signals = df['fast_mavg'] >= df['slow_mavg'] \n",
    "short_signals = df['fast_mavg'] < df['slow_mavg'] \n",
    "df.loc[long_signals, 'side'] = 1\n",
    "df.loc[short_signals, 'side'] = -1\n",
    "\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "df['side'] = df['side'].shift(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "# Drop the NaN values from our data set\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    2393\n",
      "-1    2231\n",
      "Name: label, dtype: int64\n",
      " 1.0    2587\n",
      "-1.0    2037\n",
      "Name: side, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n",
    "print(df['side'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the input data\n",
    "_df = df.reset_index()\n",
    "trades = _df[['Date', 'Adj Close', 'Volume']].to_numpy()\n",
    "\n",
    "# define the dollar value to sample the data\n",
    "# frequency = _df.Volume.mean()*20\n",
    "frequency = df['Volume'].resample('D').sum().mean()/10.0\n",
    "\n",
    "# generate the dollar bars\n",
    "dollar_bars = bars.generate_dollarbars(trades, frequency=frequency) \n",
    "\n",
    "# define closing price\n",
    "close = dollar_bars.close.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Events using CUSUM Filter\n",
    "\n",
    "We will then predict what will happen if the event is triggered, based on the 'side' signal from the Trend Following Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Daily Volatility\n",
    "dailyVolatility = vol.getDailyVol(close, span=50)\n",
    "\n",
    "# apply cusum filter to identify events as cumulative log return passed threshold\n",
    "#tEvents = flt.getTEvents(close, h=dailyVolatility.mean()*0.5)\n",
    "tEvents = flt.cusum_filter(close, threshold=dailyVolatility.mean()*0.5)\n",
    "\n",
    "# Define vertical barrier - subjective judgment\n",
    "num_days = 1\n",
    "\n",
    "t1 = tbar.add_vertical_barrier(tEvents, close, num_days=num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get side labels from trend following method\n",
    "side_labels = []\n",
    "\n",
    "for dt in dollar_bars.index:\n",
    "    side_labels.append(df.loc[dt]['label'])\n",
    "\n",
    "side_labels = pd.Series(side_labels, index=dollar_bars.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trade or Not Trade labels using Triple Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[Timestamp('2022-12-20 15:10:00'), Timestamp('2022-12-21 09:30:00'), Timestamp('2022-12-21 10:15:00')] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     cpus \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mcpu_count() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 15\u001b[0m events \u001b[39m=\u001b[39m tbar\u001b[39m.\u001b[39;49mget_events(dollar_bars\u001b[39m.\u001b[39;49mclose, \n\u001b[0;32m     16\u001b[0m                          t_events\u001b[39m=\u001b[39;49mtEvents, \n\u001b[0;32m     17\u001b[0m                          pt_sl\u001b[39m=\u001b[39;49mptsl, \n\u001b[0;32m     18\u001b[0m                          target\u001b[39m=\u001b[39;49mdailyVolatility, \n\u001b[0;32m     19\u001b[0m                          min_ret\u001b[39m=\u001b[39;49mminRet, \n\u001b[0;32m     20\u001b[0m                          num_threads\u001b[39m=\u001b[39;49mcpus, \n\u001b[0;32m     21\u001b[0m                          vertical_barrier_times\u001b[39m=\u001b[39;49mt1,\n\u001b[0;32m     22\u001b[0m                          side_prediction\u001b[39m=\u001b[39;49mside_labels)\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\capstone_git\\MADS_Capstone\\afml\\labeling\\triplebarrier.py:125\u001b[0m, in \u001b[0;36mget_events\u001b[1;34m(close, t_events, pt_sl, target, min_ret, num_threads, vertical_barrier_times, side_prediction)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39mSnippet 3.6 page 50, Getting the Time of the First Touch, with Meta Labels\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m        -events['sl'] Stop loss multiple\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# 1) Get target\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39;49mloc[t_events]\n\u001b[0;32m    126\u001b[0m target \u001b[39m=\u001b[39m target[target \u001b[39m>\u001b[39m min_ret]  \u001b[39m# min_ret\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m# 2) Get vertical barrier (max holding period)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1434\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\choit\\OneDrive\\Coursera\\SIADS699\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[Timestamp('2022-12-20 15:10:00'), Timestamp('2022-12-21 09:30:00'), Timestamp('2022-12-21 10:15:00')] not in index\""
     ]
    }
   ],
   "source": [
    "# Define upper and lower horizontal barriers\n",
    "# set profit and stop loss ratio\n",
    "ptsl = [2,1]\n",
    "\n",
    "# select minRet\n",
    "minRet = 0.01 # requires at least 1 percent return\n",
    "\n",
    "# Run in single-threaded mode on Windows\n",
    "import platform, os\n",
    "if platform.system() == \"Windows\":\n",
    "    cpus = 1\n",
    "else:\n",
    "    cpus = os.cpu_count() - 1\n",
    "    \n",
    "events = tbar.get_events(dollar_bars.close, \n",
    "                         t_events=tEvents, \n",
    "                         pt_sl=ptsl, \n",
    "                         target=dailyVolatility, \n",
    "                         min_ret=minRet, \n",
    "                         num_threads=cpus, \n",
    "                         vertical_barrier_times=t1,\n",
    "                         side_prediction=side_labels).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tbar.get_bins(triple_barrier_events = events, close=close)\n",
    "\n",
    "print(labels.bin.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop underpopulated labels\n",
    "clean_labels  = tbar.drop_labels(labels)\n",
    "\n",
    "print(clean_labels.bin.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels[clean_labels['bin']==1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Primary Model without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(clean_labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a Meta model\n",
    "\n",
    "Use features: volatility, serial correlation, relative strength to S&P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['Adj Close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['Adj Close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['Adj Close'].pct_change(periods=2)\n",
    "raw_data['mom3'] = raw_data['Adj Close'].pct_change(periods=3)\n",
    "raw_data['mom4'] = raw_data['Adj Close'].pct_change(periods=4)\n",
    "raw_data['mom5'] = raw_data['Adj Close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=1, center=False).std()\n",
    "raw_data['volatility_31'] = raw_data['log_ret'].rolling(window=31, min_periods=1, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=1, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=5), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t3'] = raw_data['log_ret'].shift(3)\n",
    "raw_data['log_t4'] = raw_data['log_ret'].shift(4)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re compute sides\n",
    "# raw_data['side'] = np.nan\n",
    "\n",
    "# long_signals = raw_data['fast_mavg'] >= raw_data['slow_mavg']\n",
    "# short_signals = raw_data['fast_mavg'] < raw_data['slow_mavg']\n",
    "\n",
    "# raw_data.loc[long_signals, 'side'] = 1\n",
    "# raw_data.loc[short_signals, 'side'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial correlation\n",
    "srl_corr = mkt.df_rolling_autocorr(mkt.returns(close), window=7).rename('srl_corr')\n",
    "\n",
    "# relative strength to SPY\n",
    "rs_SPY = mkt.get_relative_strength(df['Adj Close'], index_SPY['Adj Close']).shift(1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractional differentiated log dollar bar prices\n",
    "\n",
    "# cumulative sum of stock log-prices\n",
    "ticker_log_cumsum = np.log(dollar_bars.close).cumsum()\n",
    "\n",
    "# frac diff 1 time\n",
    "dfx1 = fdiff.frac_diff_ffd(ticker_log_cumsum.to_frame(), diff_amt=1).dropna()\n",
    "\n",
    "# apply cumsum filter\n",
    "dfx1_close = dfx1.close.copy()\n",
    "df_tEvents = flt.getTEvents(dfx1_close, h=dfx1.std().iat[0]*2)\n",
    "\n",
    "# fracDiff value feature\n",
    "frac_diff_feat = dfx1.loc[df_tEvents] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (pd.DataFrame()\n",
    "            .assign(vol=events.trgt)\n",
    "            #.assign(side=clean_labels.side)\n",
    "            #.assign(srl_corr=srl_corr)\n",
    "            .assign(rs_SPY=rs_SPY)\n",
    "            #.assign(frac_diff_feat=frac_diff_feat)\n",
    "            .drop_duplicates()\n",
    "            .dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "_X = raw_data.loc[clean_labels.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "_X.drop(['Ticker',\n",
    "        'Adj Close', 'High', 'Low', 'Open', 'Volume', \n",
    "        #'fast_mavg', 'slow_mavg', 'side', # remove for MA crossover\n",
    "        #'slope', 'label', # remove for trend scanning\n",
    "        ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = _X.join(features).join(clean_labels['bin']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xy.iloc[:, :-1]\n",
    "y = Xy.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_based_sample_weights = get_weights_by_return(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'])\n",
    "time_based_sample_weights = get_weights_by_time_decay(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'], decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.title('Return based sample weights')\n",
    "return_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.title('Time decaying sample weights')\n",
    "time_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[2, 3, 4, 5, 7],\n",
    "              'n_estimators':[10, 25, 50, 100, 256, 512]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gen_standard = KFold(4)\n",
    "cv_gen_purged = PurgedKFold(n_splits=4, samples_info_sets=events.loc[X_train.index].t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(X_data, y_data, cv_gen, scoring, type='standard'):\n",
    "    \"\"\"\n",
    "    Grid search using Purged CV without using sample weights in fit(). Returns top model and top score\n",
    "    \"\"\"\n",
    "    max_cross_val_score = -np.inf\n",
    "    top_model = None\n",
    "    for m_depth in parameters['max_depth']:\n",
    "        for n_est in parameters['n_estimators']:\n",
    "            clf_base = DecisionTreeClassifier(criterion='entropy', random_state=42, \n",
    "                                              max_depth=m_depth, class_weight='balanced')\n",
    "            clf_bagging = BaggingClassifier(n_estimators=n_est, \n",
    "                                        estimator=clf_base, \n",
    "                                        random_state=42, n_jobs=-1, \n",
    "                                        oob_score=False, max_features=1.)\n",
    "            clf_random_forest = RandomForestClassifier(n_estimators=n_est, \n",
    "                                             max_depth=m_depth, \n",
    "                                             random_state=42, \n",
    "                                             n_jobs=-1, \n",
    "                                             oob_score=False, \n",
    "                                            criterion='entropy',\n",
    "                                            class_weight='balanced_subsample', \n",
    "                                            max_features=1.)\n",
    "\n",
    "            if type == 'standard':\n",
    "                clf = clf_bagging\n",
    "            elif type == 'random_forest':\n",
    "                clf = clf_random_forest\n",
    "            elif type == 'sequential_bootstrapping':\n",
    "                clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, ## events\n",
    "                                                                price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], ## df\n",
    "                                                                n_estimators=n_est, estimator=clf_base, \n",
    "                                                                random_state=42, n_jobs=-1, oob_score=False,\n",
    "                                                                max_features=1.)\n",
    "            temp_score_base = ml_cross_val_score(clf, X_data, y_data, cv_gen, scoring=scoring)\n",
    "            if temp_score_base.mean() > max_cross_val_score:\n",
    "                max_cross_val_score = temp_score_base.mean()\n",
    "                print(temp_score_base.mean())\n",
    "                top_model = clf\n",
    "    return top_model, max_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_sample_weights(X_data, y_data, cv_gen, scoring, type='standard'):\n",
    "    \"\"\"\n",
    "    Grid search using Purged CV using sample weights in fit(). Returns top model and top score\n",
    "    \"\"\"\n",
    "    max_cross_val_score = -np.inf\n",
    "    top_model = None\n",
    "    for m_depth in parameters['max_depth']:\n",
    "        for n_est in parameters['n_estimators']:\n",
    "            clf_base = DecisionTreeClassifier(criterion='entropy', random_state=42, \n",
    "                                              max_depth=m_depth, class_weight='balanced')\n",
    "            clf_bagging = BaggingClassifier(n_estimators=n_est, \n",
    "                                        estimator=clf_base, \n",
    "                                        random_state=42, n_jobs=-1, \n",
    "                                        oob_score=False, max_features=1.)\n",
    "            clf_random_forest = RandomForestClassifier(n_estimators=n_est, \n",
    "                                             max_depth=m_depth, \n",
    "                                             random_state=42, \n",
    "                                             n_jobs=-1, \n",
    "                                             oob_score=False, \n",
    "                                            criterion='entropy',\n",
    "                                            class_weight='balanced_subsample', \n",
    "                                            max_features=1.)\n",
    "\n",
    "            if type == 'standard':\n",
    "                clf = clf_bagging\n",
    "            elif type == 'random_forest':\n",
    "                clf = clf_random_forest\n",
    "            elif type == 'sequential_bootstrapping':\n",
    "                clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, #events\n",
    "                                                                price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], #df\n",
    "                                                                n_estimators=n_est, estimator=clf_base, \n",
    "                                                                random_state=42, n_jobs=-1, oob_score=False,\n",
    "                                                                max_features=1.)\n",
    "            temp_score_base = ml_cross_val_score(clf, X_data, y_data, cv_gen, scoring=scoring,\n",
    "                                                                    sample_weight=return_based_sample_weights.values)\n",
    "            if temp_score_base.mean() > max_cross_val_score:\n",
    "                max_cross_val_score = temp_score_base.mean()\n",
    "                print(temp_score_base.mean())\n",
    "                top_model = clf\n",
    "    return top_model, max_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', type = 'standard')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search_sample_weights(X_train, y_train, cv_gen_purged, 'f1', type = 'standard')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', type = 'random_forest')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search_sample_weights(X_train, y_train, cv_gen_purged, 'f1', type = 'random_forest')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', type = 'sequential_bootstrapping')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model, cross_val_score = perform_grid_search_sample_weights(X_train, y_train, cv_gen_purged, 'f1', type = 'sequential_bootstrapping')\n",
    "print(top_model, cross_val_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDI, MDA, SFI feature importance\n",
    "mdi_feat_imp = mean_decrease_impurity(top_model, X_train.columns)\n",
    "mda_feat_imp = mean_decrease_accuracy(top_model, X_train, y_train, cv_gen_purged, scoring='f1', \n",
    "                                                         sample_weight=return_based_sample_weights.values)\n",
    "sfi_feat_imp = single_feature_importance(top_model, X_train, y_train, cv_gen_purged, scoring='f1',\n",
    "                                     sample_weight=return_based_sample_weights.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(mdi_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(mda_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(sfi_feat_imp, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = top_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = top_model.predict(X_test)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator = 10000\n",
    "rf = RandomForestClassifier(max_depth=2, \n",
    "                            n_estimators=n_estimator,\n",
    "                            criterion='entropy', \n",
    "                            class_weight='balanced',\n",
    "                            random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = rf.predict(X_test)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
