{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy.trendlabeling as tlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getdata as gd\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "\n",
    "import afml.filters.filters as flt \n",
    "import afml.labeling.triplebarrier as tbar\n",
    "import afml.util.volatility as vol\n",
    "import features.bars as bars  \n",
    "import features.marketindicators as mkt\n",
    "import afml.features.fracdiff as fdiff\n",
    "from afml.ensemble.sb_bagging import SequentiallyBootstrappedBaggingClassifier\n",
    "from afml.sample_weights.attribution import get_weights_by_return, get_weights_by_time_decay\n",
    "from afml.feature_importance.importance import mean_decrease_impurity, mean_decrease_accuracy, single_feature_importance, plot_feature_importance\n",
    "from afml.cross_validation.cross_validation import PurgedKFold, ml_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yf = gd.get_yf_data(tickers= \"SPY COMP ALGM\", \n",
    "#                     period='60d',   \n",
    "#                     interval='5m'\n",
    "# )\n",
    "\n",
    "# df = df_yf[df_yf['Ticker'] == 'ALGM']\n",
    "# index_SPY = df_yf[df_yf['Ticker'] == 'SPY']\n",
    "# index_COMP = df_yf[df_yf['Ticker'] == 'COMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataframe\n",
    "\n",
    "yf.pdr_override()\n",
    "\n",
    "def get_yf_daily(ticker, startdate, enddate, years):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(years):\n",
    "        start = startdate.replace(startdate.year-i).strftime('%Y-%m-%d')\n",
    "        end = enddate.replace(enddate.year-i).strftime('%Y-%m-%d')\n",
    "        thisdf = pdr.get_data_yahoo(ticker, start=start, end=end)\n",
    "        df= pd.concat([df, thisdf], axis= 0)\n",
    "        print('Complete {} {}'.format(ticker, enddate.year-i))\n",
    "    return df\n",
    "\n",
    "years = 10\n",
    "startdate= datetime.date(2021, 1, 1)\n",
    "enddate = datetime.date(2021, 12, 31)\n",
    "\n",
    "df = get_yf_daily('STBA', startdate, enddate, years).sort_index(ascending=True)\n",
    "index_SPY = get_yf_daily('SPY', startdate, enddate, years).sort_index(ascending=True)\n",
    "#index_COMP = get_yf_daily('COMP', startdate, enddate, years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ticker.csv')\n",
    "index_SPY.to_csv('spy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ticker.csv')\n",
    "index_SPY = pd.read_csv('spy.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "# Drop the NaN values from our data set\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "index_SPY.dropna(axis=0, how='any', inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the input data\n",
    "_df = df.reset_index()\n",
    "trades = _df[['Date', 'Adj Close', 'Volume']].to_numpy()\n",
    "\n",
    "# define the dollar value to sample the data\n",
    "frequency = _df.Volume.mean()*20\n",
    "#frequency = df['Volume'].resample('D').sum().mean()/10.0\n",
    "\n",
    "# generate the dollar bars\n",
    "dollar_bars = bars.generate_dollarbars(trades, frequency=frequency) \n",
    "\n",
    "# define closing price\n",
    "close = dollar_bars.close.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Trend Scanning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Adj Close to numpy\n",
    "#time_series = df['Adj Close'].to_numpy()\n",
    "time_series = df['Adj Close'].to_numpy()\n",
    "window_size_max= 7\n",
    "\n",
    "# get trend scanning labels\n",
    "label_output = pd.DataFrame(tlb.get_trend_scanning_labels(time_series=time_series, \n",
    "                                             window_size_max=window_size_max, \n",
    "                                             threshold=0.0,\n",
    "                                             opp_sign_ct=3,\n",
    "                                             side='both'),\n",
    "                                        index= df.index[window_size_max-1:])\n",
    "\n",
    "# drop last rolling window size -1 rows\n",
    "# n = window_size_max-1\n",
    "# df.drop(df.tail(n).index, inplace = True)\n",
    "# df = df.iloc[:-n]\n",
    "df = df.join(label_output, how='outer')\n",
    "\n",
    "\n",
    "\n",
    "# append the slope and labels to the df\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "df['slope'] = df['slope'].shift(1)\n",
    "df['label'] = df['label'].shift(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labels: Simple moving average cross over strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to cite here ...\n",
    "\n",
    "# compute moving averages\n",
    "fast_window = 20\n",
    "slow_window = 50\n",
    "\n",
    "df['fast_mavg'] = df['Adj Close'].rolling(window=fast_window, min_periods=fast_window, center=False).mean()\n",
    "df['slow_mavg'] = df['Adj Close'].rolling(window=slow_window, min_periods=slow_window, center=False).mean()\n",
    "df.head()\n",
    "\n",
    "# Compute sides\n",
    "df['side'] = np.nan\n",
    "\n",
    "long_signals = df['fast_mavg'] >= df['slow_mavg'] \n",
    "short_signals = df['fast_mavg'] < df['slow_mavg'] \n",
    "df.loc[long_signals, 'side'] = 1\n",
    "df.loc[short_signals, 'side'] = -1\n",
    "\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "df['side'] = df['side'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['label'].value_counts())\n",
    "print(df['side'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Events using CUSUM Filter\n",
    "\n",
    "We will then predict what will happen if the event is triggered, based on the 'side' signal from the Trend Following Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Daily Volatility\n",
    "dailyVolatility = vol.getDailyVol(close, span=50)\n",
    "\n",
    "# apply cusum filter to identify events as cumulative log return passed threshold\n",
    "#tEvents = flt.getTEvents(close, h=dailyVolatility.mean()*0.5)\n",
    "tEvents = flt.cusum_filter(close, threshold=dailyVolatility.mean()*0.5, signal=None)\n",
    "\n",
    "# Define vertical barrier - subjective judgment\n",
    "num_days = 10\n",
    "\n",
    "t1 = tbar.add_vertical_barrier(tEvents, close, num_days=num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get side labels from trend following method\n",
    "side_labels = []\n",
    "\n",
    "for dt in dollar_bars.index:\n",
    "    side_labels.append(df.loc[dt]['label'])\n",
    "\n",
    "side_labels = pd.Series(side_labels, index=dollar_bars.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trade or Not Trade labels using Triple Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upper and lower horizontal barriers\n",
    "# set profit and stop loss ratio\n",
    "ptsl = [2,1]\n",
    "\n",
    "# select minRet\n",
    "minRet = 0.01 # requires at least 1 percent return\n",
    "\n",
    "# Run in single-threaded mode on Windows\n",
    "import platform, os\n",
    "if platform.system() == \"Windows\":\n",
    "    cpus = 1\n",
    "else:\n",
    "    cpus = os.cpu_count() - 1\n",
    "    \n",
    "events = tbar.get_events(dollar_bars.close, \n",
    "                         t_events=tEvents, \n",
    "                         pt_sl=ptsl, \n",
    "                         target=dailyVolatility, \n",
    "                         min_ret=minRet, \n",
    "                         num_threads=cpus, \n",
    "                         vertical_barrier_times=t1,\n",
    "                         side_prediction=side_labels).dropna()\n",
    "\n",
    "labels = tbar.get_bins(triple_barrier_events = events, close=close)\n",
    "\n",
    "# Drop underpopulated labels\n",
    "clean_labels  = tbar.drop_labels(labels)\n",
    "print(clean_labels.bin.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Secondary Model without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(clean_labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a Meta model\n",
    "\n",
    "Use features: volatility, serial correlation, relative strength to S&P and COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = df.drop(columns='Close').copy()\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['Adj Close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['Adj Close'].pct_change(periods=1).shift(1)\n",
    "raw_data['mom2'] = raw_data['Adj Close'].pct_change(periods=2).shift(1)\n",
    "raw_data['mom3'] = raw_data['Adj Close'].pct_change(periods=3).shift(1)\n",
    "raw_data['mom4'] = raw_data['Adj Close'].pct_change(periods=4).shift(1)\n",
    "raw_data['mom5'] = raw_data['Adj Close'].pct_change(periods=5).shift(1)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=3, center=False).std().shift(1)\n",
    "raw_data['volatility_31'] = raw_data['log_ret'].rolling(window=31, min_periods=3, center=False).std().shift(1)\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=3, center=False).std().shift(1)\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=1), raw=False).shift(1)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=2), raw=False).shift(1)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=3), raw=False).shift(1)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=4), raw=False).shift(1)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=3, center=False).apply(lambda x: x.autocorr(lag=5), raw=False).shift(1)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t3'] = raw_data['log_ret'].shift(3)\n",
    "raw_data['log_t4'] = raw_data['log_ret'].shift(4)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)\n",
    "\n",
    "#Correct lookahead bias\n",
    "raw_data['fast_mavg'] = raw_data['fast_mavg'].shift(1)\n",
    "raw_data['slow_mavg'] = raw_data['slow_mavg'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re compute sides\n",
    "# raw_data['side'] = np.nan\n",
    "\n",
    "# long_signals = raw_data['fast_mavg'] >= raw_data['slow_mavg']\n",
    "# short_signals = raw_data['fast_mavg'] < raw_data['slow_mavg']\n",
    "\n",
    "# raw_data.loc[long_signals, 'side'] = 1\n",
    "# raw_data.loc[short_signals, 'side'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial correlation\n",
    "srl_corr = mkt.df_rolling_autocorr(mkt.returns(close), window=7).rename('srl_corr')\n",
    "\n",
    "# relative strength to SPY\n",
    "rs_SPY = mkt.get_relative_strength(df['Adj Close'], index_SPY['Adj Close']).shift(1).dropna()\n",
    "\n",
    "# relative strength to COMP\n",
    "#rs_COMP = mkt.get_relative_strength(df['Adj Close'], index_COMP['Adj Close']).shift(1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractional differentiated log dollar bar prices\n",
    "\n",
    "# cumulative sum of stock log-prices\n",
    "ticker_log_cumsum = np.log(dollar_bars.close).cumsum()\n",
    "\n",
    "# frac diff 1 time\n",
    "dfx1 = fdiff.frac_diff_ffd(ticker_log_cumsum.to_frame(), diff_amt=1).dropna()\n",
    "\n",
    "# apply cumsum filter\n",
    "dfx1_close = dfx1.close.copy()\n",
    "df_tEvents = flt.getTEvents(dfx1_close, h=dfx1.std().iat[0]*2)\n",
    "\n",
    "# fracDiff value feature\n",
    "frac_diff_feat = dfx1.loc[df_tEvents] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (pd.DataFrame()\n",
    "            #.assign(vol=events.trgt)\n",
    "            #.assign(side=clean_labels.side)\n",
    "            #.assign(srl_corr=srl_corr)\n",
    "            .assign(rs_SPY=rs_SPY)\n",
    "            #.assign(rs_COMP=rs_COMP)\n",
    "            #.assign(frac_diff_feat=frac_diff_feat)\n",
    "            .drop_duplicates()\n",
    "            .dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "_X = raw_data.loc[clean_labels.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "_X.drop([#'Ticker',\n",
    "        'Adj Close', 'High', 'Low', 'Open', 'Volume', 'log_ret',\n",
    "        #'fast_mavg', 'slow_mavg', 'side', # remove for MA crossover\n",
    "        #'slope', 'label', # remove for trend scanning\n",
    "        ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = _X.join(features).join(clean_labels['bin']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xy.iloc[:, :-1]\n",
    "y = Xy.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data sample weights\n",
    "\n",
    "return_based_sample_weights = get_weights_by_return(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'])\n",
    "time_based_sample_weights = get_weights_by_time_decay(events.loc[X_train.index], df.loc[X_train.index, 'Adj Close'], decay=0.5)\n",
    "\n",
    "# test data sample weights\n",
    "\n",
    "return_based_sample_weights_test = get_weights_by_return(events.loc[X_test.index], df.loc[X_test.index, 'Adj Close'])\n",
    "time_based_sample_weights_test = get_weights_by_time_decay(events.loc[X_test.index], df.loc[X_test.index, 'Adj Close'], decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Return based sample weights')\n",
    "return_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.title('Time decaying sample weights')\n",
    "time_based_sample_weights.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring, sample_weight, scaler=StandardScaler()):\n",
    "    t0 = 0.0\n",
    "    t1 = 0.0\n",
    "\n",
    "    best_param_dict = {}\n",
    "    best_param_dict['type'] = type\n",
    "    best_param_dict['top_model'] = None\n",
    "    best_param_dict['max_cross_val_score'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_recall'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_precision'] = -np.inf\n",
    "    best_param_dict['max_cross_val_score_accuracy'] = -np.inf\n",
    "    best_param_dict['run_time'] = 0.0\n",
    "\n",
    "    col = X_train.columns.to_list()\n",
    "    idx = X_train.index\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=col, index=idx)\n",
    "\n",
    "    t0 = time.time()\n",
    "    temp_score_base, temp_recall, temp_precision, temp_accuracy = ml_cross_val_score(clf, X_train_scaled, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    if temp_score_base.mean() > best_param_dict['max_cross_val_score']:\n",
    "        best_param_dict['top_model'] = clf\n",
    "        best_param_dict['max_cross_val_score'] = temp_score_base.mean()\n",
    "        best_param_dict['max_cross_val_score_recall'] = temp_recall.mean()\n",
    "        best_param_dict['max_cross_val_score_precision'] = temp_precision.mean()\n",
    "        best_param_dict['max_cross_val_score_accuracy'] = temp_accuracy.mean()\n",
    "        best_param_dict['run_time'] = t1-t0    \n",
    "    \n",
    "    return best_param_dict\n",
    "\n",
    "def perform_grid_search(X_train, y_train, cv_gen, scoring, parameters, events, type='standard', sample_weight=None, RANDOM_STATE=42):\n",
    "    \"\"\"\n",
    "    Grid search using Purged CV without using sample weights in fit(). Returns top model and top score\n",
    "    \"\"\"\n",
    "\n",
    "    if type=='SVC' or type=='seq_boot_SVC':\n",
    "        for C in parameters['C']:\n",
    "            for gamma in parameters['gamma']:\n",
    "\n",
    "                clf_SVC = SVC(C=C,\n",
    "                                gamma=gamma,\n",
    "                                class_weight='balanced',\n",
    "                                kernel='linear',\n",
    "                                random_state=RANDOM_STATE)\n",
    "\n",
    "                if type =='SVC':\n",
    "                    clf = clf_SVC\n",
    "                elif type == 'seq_boot_SVC':\n",
    "                    clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, ## events\n",
    "                                                                price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], ## df\n",
    "                                                                estimator=clf_SVC, \n",
    "                                                                random_state=RANDOM_STATE, n_jobs=-1, oob_score=False,\n",
    "                                                                max_features=1.)\n",
    "\n",
    "                # get best param dict   \n",
    "                best_param_dict = get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "    else:    \n",
    "        for m_depth in parameters['max_depth']:\n",
    "            for n_est in parameters['n_estimators']:\n",
    "                clf_base = DecisionTreeClassifier(criterion='entropy', random_state=RANDOM_STATE, \n",
    "                                                max_depth=m_depth, class_weight='balanced')\n",
    "\n",
    "                if type == 'standard_bagging':\n",
    "                    clf = BaggingClassifier(n_estimators=n_est, \n",
    "                                            estimator=clf_base, \n",
    "                                            random_state=RANDOM_STATE, n_jobs=-1, \n",
    "                                            oob_score=False, max_features=1.)\n",
    "                elif type == 'random_forest':\n",
    "                    clf = RandomForestClassifier(n_estimators=n_est, \n",
    "                                                max_depth=m_depth, \n",
    "                                                random_state=RANDOM_STATE, \n",
    "                                                n_jobs=-1, \n",
    "                                                oob_score=False, \n",
    "                                                criterion='entropy',\n",
    "                                                class_weight='balanced_subsample', \n",
    "                                                max_features=1.)\n",
    "                elif type == 'sequential_bootstrapping':\n",
    "                    clf = SequentiallyBootstrappedBaggingClassifier(samples_info_sets=events.loc[X_train.index].t1, ## events\n",
    "                                                                    price_bars = dollar_bars.loc[X_train.index.min():X_train.index.max(), 'close'], ## df\n",
    "                                                                    estimator=clf_base, \n",
    "                                                                    n_estimators=n_est, \n",
    "                                                                    random_state=RANDOM_STATE, \n",
    "                                                                    n_jobs=-1, \n",
    "                                                                    oob_score=False,\n",
    "                                                                    max_features=1.)\n",
    "                \n",
    "                # get best param dict   \n",
    "                best_param_dict = get_clf_best_param_cv(type, clf, X_train, y_train, cv_gen, scoring=scoring, sample_weight=sample_weight)\n",
    "\n",
    "    return best_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[2, 3, 4, 5, 7],\n",
    "              'n_estimators':[10, 25, 50, 100, 256, 512],\n",
    "              'C':[1,10,100,1000],\n",
    "              'gamma':[1,0.1,0.001,0.0001], \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=4\n",
    "\n",
    "cv_gen_standard = KFold(n_splits)\n",
    "cv_gen_purged = PurgedKFold(n_splits=n_splits, samples_info_sets=events.loc[X_train.index].t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_models = ['standard_bagging', \n",
    "                   'random_forest', \n",
    "                   'sequential_bootstrapping', \n",
    "                   'SVC', \n",
    "                   'seq_boot_SVC'\n",
    "                   ]\n",
    "\n",
    "model_metrics = pd.DataFrame(columns = ['type', 'top_model', 'max_cross_val_score', 'max_cross_val_score_recall', 'max_cross_val_score_precision', 'max_cross_val_score_accuracy','run_time'])\n",
    "\n",
    "\n",
    "for clf in selected_models:\n",
    "    best_params = perform_grid_search(X_train, y_train, cv_gen_purged, 'f1', parameters, type=clf, sample_weight=return_based_sample_weights.values)\n",
    "    model_metrics = model_metrics.append(best_params, ignore_index = True)  \n",
    "    print('Completed {}'.format(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = model_metrics.sort_values(['max_cross_val_score']).tail(1)['top_model'].squeeze()\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVC = model_metrics[model_metrics['type'] == 'seq_boot_SVC']['top_model'].squeeze()\n",
    "best_SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC classifier does not provide a direct wat to obtain feature importances. However, we can use the coefficients of the hyperplane that seperates the classes to estimate the importance of each feature. The magnitude of the coefficient corresponds to the importance of the corresponding feature in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients of hyperplane\n",
    "coef = best_SVC.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients of hyperplane\n",
    "coef = top_model.coef_.ravel()\n",
    "indices = np.argsort(coef)\n",
    "coef = coef[indices]\n",
    "feature_names = X_train.columns.to_list()\n",
    "feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Plot the coefficients as horizontal bars\n",
    "plt.barh(range(len(coef)), coef, color='b')\n",
    "\n",
    "# Add a horizontal line to indicate standard deviation\n",
    "plt.plot([0, 0], [len(coef), -1], 'r--', lw=2)\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(range(len(coef)), feature_names)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This codes below are for tress based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MDI, MDA, SFI feature importance\n",
    "# mdi_feat_imp = mean_decrease_impurity(top_model, X_train.columns)\n",
    "# mda_feat_imp = mean_decrease_accuracy(top_model, X_train, y_train, cv_gen_purged, scoring='f1', sample_weight=sw_train)\n",
    "# sfi_feat_imp = single_feature_importance(top_model, X_train, y_train, cv_gen_purged, scoring='f1', sample_weight=sw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(mdi_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(mda_feat_imp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(sfi_feat_imp, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "#best_SVC_fitted = best_SVC.fit(X_train, y_train, sample_weight=return_based_sample_weights.values)\n",
    "#y_pred = best_SVC_fitted.predict(X_test)\n",
    "y_pred = best_SVC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "SVC_ROC = RocCurveDisplay.from_estimator(best_SVC, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = top_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(top_model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_estimator(top_model, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = X_test[['slope', 'label']]\n",
    "test_result_df['y_test'] = y_test\n",
    "test_result_df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df[test_result_df['label']==1].groupby('y_test').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "   \n",
    "   # Load the data\n",
    "data = pd.read_csv('your_data.csv')\n",
    "   \n",
    "   # Calculate the daily returns\n",
    "data['daily_returns'] = data['log_returns'].apply(lambda x: math.exp(x) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transaction cost and slippage parameters\n",
    "transaction_cost = 0.01  # 1% transaction cost\n",
    "slippage = 0.005  # 0.5% slippage\n",
    "\n",
    "# Calculate the trade returns\n",
    "data['trade_returns'] = data['daily_returns'] * data['trade'].shift(1) - abs(data['trade'].diff()) * (transaction_cost + slippage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative returns\n",
    "data['cumulative_returns'] = (data['trade_returns'] + 1).cumprod() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart of the cumulative returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "   # Visualize the results\n",
    "plt.plot(data['date'], data['cumulative_returns'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.title('Backtest Results')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
